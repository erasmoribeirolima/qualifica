{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carrega o dataset\n",
    "dados = pd.read_csv('/home/erasmor/Área de Trabalho/CSE-CIC-IDS2018/todos_apenas_baixa_representatividade.csv',sep=\",\",encoding = 'utf-8',  header=0,na_values='.',dtype={'Label':'category'})\n",
    "\n",
    "#remove valores infinitos\n",
    "dados.replace(-np.Inf, np.nan)\n",
    "\n",
    "#substitui valores NaN\n",
    "dados.fillna(dados.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mostra como está a base de dados\n",
    "#dados.head()\n",
    "dados.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifica quantas instâncias (linhas) e quantos atributos (colunas) a base de dados contém\n",
    "print(\"numero de linhas e colunas: \",dados.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizar distribições por classes contidas no csv - informar nome da classe alvo\n",
    "print(dados.groupby('Label').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_raw_normalize = MinMaxScaler(X_raw_normalize.reshape(0, 1)).reshape(len(X_raw_normalize))\n",
    "#X_raw_normalizetd2 = (X_raw_normalize - X_raw_normalize.min(axis=0)) / (X_raw_normalize.max(axis=0) - X_raw_normalize.min(axis=0))\n",
    "# Obtendo os nomes das colunas do DataFrame como uma lista.\n",
    "cols = list(dados.columns)\n",
    "# colunas que nao serao normalizadas\n",
    "cols.remove('Label')\n",
    "# Copiando os dados e aplicando a normalizacao por reescala nas colunas do DataFrame que contem\n",
    "# valores continuos. Por padrao, o metodo minmax_scale reescala com min=0 e max=1.\n",
    "dados = dados[~dados.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "dados[cols] = dados[cols].apply(minmax_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define as colunas de atributos e a coluna da classe (de 0 a 72 são atributos e após a 72 é a classe)\n",
    "# \"X_raw\" é features/atributos e \"y_raw\" é target/classe ==> As duas formas abaixo dão certo.\n",
    "#array = dataset.values\n",
    "#X_raw = array[:,0:72]\n",
    "#y_raw = array[:,72]\n",
    "X_raw = dados.iloc[:, :-1].values # atributos\n",
    "y_raw = dados.iloc[:, 78].values # classe de ataques\n",
    "X_raw = np.nan_to_num(X_raw.astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformar a variável Y com valores categóricos das classses de ataques em valores:\n",
    "labelencoder_y = LabelEncoder()\n",
    "y_raw = labelencoder_y.fit_transform(y_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando um PCA. O parametro n_components indica a quantidade de dimensoes que a base\n",
    "# original sera reduzida.\n",
    "pca = PCA(n_components=10, whiten=True,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando o pca na base de dados. O atributo 'values' retorna um numpy.array\n",
    "# de duas dimensões (matriz) contendo apenas os valores numericos do DataFrame.\n",
    "X_raw = pca.fit_transform(X_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def committee_sampling_knn(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, classifier, t_inicial):\n",
    "    \n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "    import functools\n",
    "    from modAL.uncertainty import classifier_uncertainty\n",
    "    from modAL.models import ActiveLearner, Committee\n",
    "    from modAL.uncertainty import uncertainty_sampling\n",
    "    from modAL.disagreement import vote_entropy_sampling\n",
    "    from IPython.display import clear_output\n",
    "    \n",
    "        \n",
    "    #define nome de arquivos para salvar\n",
    "    indica_pool=str(idx_dobra)\n",
    "    \n",
    "    # recupera as amostras de treino iniciais - a extratificação realizada só serve para tal finalidade.\n",
    "    # No caso força a buscar pelo menos uma amostras de cada rótulo disponível (train_size= len(np.unique(y_raw)).\n",
    "    # Realizar a busca aleatoriamente não garante iniciar com uma instância de cada classe.\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]], train_size= len(np.unique(y_raw)) + t_inicial, stratify = y_raw[idx_data[idx_dobra][TRAIN]])\n",
    "    #print(\"tamanho de X_train inicial: \",X_train.shape)\n",
    "    \n",
    "    # recupera amostras de teste de acordo com a dobra em uso\n",
    "    X_teste, y_teste = X_raw[idx_data[idx_dobra][TEST]], y_raw[idx_data[idx_dobra][TEST]]\n",
    "    # recupera amostras de treino (bruto) de acordo com a dobra em uso\n",
    "    X_treino, y_treino = X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]]\n",
    "    \n",
    "    #isola exemplos para treinamento inicial\n",
    "    n_labeled_examples = X_treino.shape[0]\n",
    "    training_indices = np.random.randint(low=0, high=n_labeled_examples + 1, size=5)\n",
    "    \n",
    "    X_train = X_treino[training_indices]\n",
    "    y_train = y_treino[training_indices]\n",
    "    \n",
    "    #Isole os exemplos do pool para consultar\n",
    "    \n",
    "    X_pool = np.delete(X_treino, training_indices, axis=0)\n",
    "    y_pool = np.delete(y_treino, training_indices, axis=0)\n",
    "    \n",
    "    # initializing Committee members\n",
    "    n_members = 3\n",
    "    learner_list = list()\n",
    "    \n",
    "    #instancia classificadores de aprendizado ativo\n",
    "    learner_knn = ActiveLearner(estimator=KNeighborsClassifier(n_neighbors=5),X_training=X_train, y_training=y_train)\n",
    "    learner_list.append(learner_knn)\n",
    "    arquivo_performance_knn = open(\"committee_performance_knn_dobra_\"+indica_pool+\".txt\",\"a\")\n",
    "    arquivo_history_knn = (\"committee_history_knn_dobra_\"+indica_pool+\".csv\")\n",
    "              \n",
    "    learner_list.append(learner_knn)\n",
    "        \n",
    "    # assembling the committee\n",
    "    committee = Committee(learner_list=learner_list,query_strategy=vote_entropy_sampling)\n",
    "    \n",
    "    #Registro da pontuação na porção de teste com o treinamento inicial\n",
    "    uncertain_sample_score_knn = committee.score(X_teste, y_teste)\n",
    "    #performance_history_knn.append(uncertain_sample_score_knn)\n",
    "    \n",
    "    # define tamanho fixo de consultas para aprendizado\n",
    "    n_queries = QUERIES\n",
    "    \n",
    "    #inicio aprendizado ativo\n",
    "    #for index in range(n_amostras_pool):\n",
    "    for index in range(n_queries):\n",
    "        #recupera amostras do pool baseado na estratégia de consulta\n",
    "        query_index, query_instance = committee.query(X_pool)\n",
    "            \n",
    "        # Ensina ao modelo ActiveLearner o registro solicitado (amostras vão para o topo).\n",
    "        committee.teach(X=X_pool[query_index].reshape(1, -1),y=y_pool[query_index].reshape(1, ))\n",
    "       \n",
    "        # apaga os modelos consultados\n",
    "        X_pool = np.delete(X_pool, query_index, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_index)\n",
    "        \n",
    "        # verifica a performance após a inclusão de dados novos \n",
    "        committee_sample_score_knn = committee.score(X_teste, y_teste)\n",
    "        predictions = committee.predict(X_teste)\n",
    "        clear_output(wait=True)\n",
    "        print('Accuracy KNN after query no. %d: %f' % (index+1, committee_sample_score_knn))\n",
    "        arquivo_performance_knn.write('Accuracy after query no. %d: %f \\n' % (index+1,committee_sample_score_knn))\n",
    "        performance_history_knn.append(committee_sample_score_knn)\n",
    "        #print ('Accuracy after query no. %d: %f' % (index+1, accuracy_score(y_test, predictions)))\n",
    "        arquivo_performance_knn.write('Accuracy after query no. %d: %f \\n' % (index+1,accuracy_score(y_teste, predictions)))\n",
    "        #print ('Precision after query no. %d: %f' % (index+1, precision_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "        arquivo_performance_knn.write('Precision after query no. %d: %f \\n' % (index+1,precision_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "        #print ('Recall after query no. %d: %f' % (index+1, recall_score(y_test, predictions, average='macro',zero_division=1)))\n",
    "        arquivo_performance_knn.write('Recall after query no. %d: %f \\n' % (index+1, recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "        #print ('F1 score after query no. %d: %f' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "        f1score= 2*((precision_score(y_teste, predictions,average='macro',zero_division=1)*recall_score(y_teste, predictions, average='macro',zero_division=1))/(precision_score(y_teste, predictions,average='macro',zero_division=1)+recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "        arquivo_performance_knn.write('F1 score after query no. %d: %f \\n' % (index+1, f1score))   \n",
    "        #print (\"========================================\")\n",
    "        arquivo_performance_knn.write('======================================== \\n')\n",
    "        #for i in range(2):\n",
    "            #clear_output(wait=True)\n",
    "            #print(i,\"Aguarde, em execução!\", flush=True)\n",
    "        \n",
    "    \n",
    "    \n",
    "    np.savetxt(arquivo_history_knn, performance_history_knn,delimiter=\",\")\n",
    "    arquivo_performance_knn.close()\n",
    "    \n",
    "def committee_sampling_rf(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, classifier, t_inicial):\n",
    "    \n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "    import functools\n",
    "    from modAL.uncertainty import classifier_uncertainty\n",
    "    from modAL.models import ActiveLearner, Committee\n",
    "    from modAL.uncertainty import uncertainty_sampling\n",
    "    from modAL.disagreement import vote_entropy_sampling\n",
    "    from IPython.display import clear_output\n",
    "    \n",
    "        \n",
    "    #define nome de arquivos para salvar\n",
    "    indica_pool=str(idx_dobra)\n",
    "    \n",
    "    # recupera as amostras de treino iniciais - a extratificação realizada só serve para tal finalidade.\n",
    "    # No caso força a buscar pelo menos uma amostras de cada rótulo disponível (train_size= len(np.unique(y_raw)).\n",
    "    # Realizar a busca aleatoriamente não garante iniciar com uma instância de cada classe.\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]], train_size= len(np.unique(y_raw)) + t_inicial, stratify = y_raw[idx_data[idx_dobra][TRAIN]])\n",
    "    #print(\"tamanho de X_train inicial: \",X_train.shape)\n",
    "\n",
    "        # recupera amostras de teste de acordo com a dobra em uso\n",
    "    X_teste, y_teste = X_raw[idx_data[idx_dobra][TEST]], y_raw[idx_data[idx_dobra][TEST]]\n",
    "    # recupera amostras de treino (bruto) de acordo com a dobra em uso\n",
    "    X_treino, y_treino = X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]]\n",
    "    \n",
    "    #isola exemplos para treinamento inicial\n",
    "    n_labeled_examples = X_treino.shape[0]\n",
    "    training_indices = np.random.randint(low=0, high=n_labeled_examples + 1, size=5)\n",
    "    \n",
    "    X_train = X_treino[training_indices]\n",
    "    y_train = y_treino[training_indices]\n",
    "    \n",
    "    #Isole os exemplos do pool para consultar\n",
    "    \n",
    "    X_pool = np.delete(X_treino, training_indices, axis=0)\n",
    "    y_pool = np.delete(y_treino, training_indices, axis=0)\n",
    "    \n",
    "    \n",
    "    # initializing Committee members\n",
    "    n_members = 3\n",
    "    learner_list = list()\n",
    "    \n",
    "    #instancia classificadores de aprendizado ativo\n",
    "    learner_rf = ActiveLearner(estimator=RandomForestClassifier(random_state=42, n_estimators= 30, max_depth=5),X_training=X_train, y_training=y_train)\n",
    "    learner_list.append(learner_rf)\n",
    "    arquivo_performance_rf = open(\"committee_performance_rf_dobra_\"+indica_pool+\".txt\",\"a\")\n",
    "    arquivo_history_rf = (\"committee_history_rf_dobra_\"+indica_pool+\".csv\")\n",
    "              \n",
    "    learner_list.append(learner_rf)\n",
    "        \n",
    "    # assembling the committee\n",
    "    committee = Committee(learner_list=learner_list,query_strategy=vote_entropy_sampling)\n",
    "    \n",
    "    #Registro da pontuação na porção de teste com o treinamento inicial\n",
    "    uncertain_sample_score_rf = committee.score(X_teste, y_teste)\n",
    "    performance_history_rf.append(uncertain_sample_score_rf)\n",
    "        \n",
    "    # define tamanho fixo de consultas para aprendizado\n",
    "    n_queries = QUERIES\n",
    "    \n",
    "    #inicio aprendizado ativo\n",
    "    #for index in range(n_amostras_pool):\n",
    "    for index in range(n_queries):\n",
    "        #recupera amostras do pool baseado na estratégia de consulta\n",
    "        query_index, query_instance = committee.query(X_pool)\n",
    "            \n",
    "        # Ensina ao modelo ActiveLearner o registro solicitado (amostras vão para o topo).\n",
    "        committee.teach(X=X_pool[query_index].reshape(1, -1),y=y_pool[query_index].reshape(1, ))\n",
    "       \n",
    "        # apaga os modelos consultados\n",
    "        X_pool = np.delete(X_pool, query_index, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_index)\n",
    "        \n",
    "        # verifica a performance após a inclusão de dados novos \n",
    "        committee_sample_score_rf = committee.score(X_teste, y_teste)\n",
    "        predictions = committee.predict(X_teste)\n",
    "        clear_output(wait=True)\n",
    "        print('Accuracy RF after query no. %d: %f' % (index+1, committee_sample_score_rf))\n",
    "        arquivo_performance_rf.write('Accuracy after query no. %d: %f \\n' % (index+1,committee_sample_score_rf))\n",
    "        performance_history_rf.append(committee_sample_score_rf)\n",
    "        #print ('Accuracy after query no. %d: %f' % (index+1, accuracy_score(y_test, predictions)))\n",
    "        arquivo_performance_rf.write('Accuracy after query no. %d: %f \\n' % (index+1,accuracy_score(y_teste, predictions)))\n",
    "        #print ('Precision after query no. %d: %f' % (index+1, precision_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "        arquivo_performance_rf.write('Precision after query no. %d: %f \\n' % (index+1,precision_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "        #print ('Recall after query no. %d: %f' % (index+1, recall_score(y_test, predictions, average='macro',zero_division=1)))\n",
    "        arquivo_performance_rf.write('Recall after query no. %d: %f \\n' % (index+1, recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "        #print ('F1 score after query no. %d: %f' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "        f1score= 2*((precision_score(y_teste, predictions,average='macro',zero_division=1)*recall_score(y_teste, predictions, average='macro',zero_division=1))/(precision_score(y_teste, predictions,average='macro',zero_division=1)+recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "        arquivo_performance_rf.write('F1 score after query no. %d: %f \\n' % (index+1, f1score))   \n",
    "        #print (\"========================================\")\n",
    "        arquivo_performance_rf.write('======================================== \\n')\n",
    "        #for i in range(2):\n",
    "            #clear_output(wait=True)\n",
    "            #print(i,\"Aguarde, em execução!\", flush=True)\n",
    "        \n",
    "    \n",
    "    \n",
    "    np.savetxt(arquivo_history_rf, performance_history_rf,delimiter=\",\")\n",
    "    arquivo_performance_rf.close()\n",
    "\n",
    "def committee_sampling_tree(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, classifier, t_inicial):\n",
    "    \n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "    import functools\n",
    "    from modAL.uncertainty import classifier_uncertainty\n",
    "    from modAL.models import ActiveLearner, Committee\n",
    "    from modAL.uncertainty import uncertainty_sampling\n",
    "    from modAL.disagreement import vote_entropy_sampling\n",
    "    from IPython.display import clear_output\n",
    "    \n",
    "        \n",
    "    #define nome de arquivos para salvar\n",
    "    indica_pool=str(idx_dobra)\n",
    "    \n",
    "    # recupera as amostras de treino iniciais - a extratificação realizada só serve para tal finalidade.\n",
    "    # No caso força a buscar pelo menos uma amostras de cada rótulo disponível (train_size= len(np.unique(y_raw)).\n",
    "    # Realizar a busca aleatoriamente não garante iniciar com uma instância de cada classe.\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]], train_size= len(np.unique(y_raw)) + t_inicial, stratify = y_raw[idx_data[idx_dobra][TRAIN]])\n",
    "    \n",
    "    # recupera amostras de teste de acordo com a dobra em uso\n",
    "    X_teste, y_teste = X_raw[idx_data[idx_dobra][TEST]], y_raw[idx_data[idx_dobra][TEST]]\n",
    "    # recupera amostras de treino (bruto) de acordo com a dobra em uso\n",
    "    X_treino, y_treino = X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]]\n",
    "    \n",
    "    #isola exemplos para treinamento inicial\n",
    "    n_labeled_examples = X_treino.shape[0]\n",
    "    training_indices = np.random.randint(low=0, high=n_labeled_examples + 1, size=5)\n",
    "    \n",
    "    X_train = X_treino[training_indices]\n",
    "    y_train = y_treino[training_indices]\n",
    "    \n",
    "    #Isole os exemplos do pool para consultar\n",
    "    \n",
    "    X_pool = np.delete(X_treino, training_indices, axis=0)\n",
    "    y_pool = np.delete(y_treino, training_indices, axis=0)\n",
    "    \n",
    "    # initializing Committee members\n",
    "    n_members = 3\n",
    "    learner_list = list()\n",
    "    \n",
    "    #instancia classificadores de aprendizado ativo\n",
    "    learner_tree = ActiveLearner(estimator=DecisionTreeClassifier(max_depth=5,min_samples_split=2,min_samples_leaf=2),X_training=X_train, y_training=y_train)\n",
    "    learner_list.append(learner_tree)\n",
    "    arquivo_performance_tree = open(\"committee_performance_tree_dobra_\"+indica_pool+\".txt\",\"a\")\n",
    "    arquivo_history_tree = (\"committee_history_tree_dobra_\"+indica_pool+\".csv\")\n",
    "              \n",
    "    learner_list.append(learner_tree)\n",
    "        \n",
    "    # assembling the committee\n",
    "    committee = Committee(learner_list=learner_list,query_strategy=vote_entropy_sampling)\n",
    "    \n",
    "    #Registro da pontuação na porção de teste com o treinamento inicial\n",
    "    uncertain_sample_score_tree = committee.score(X_teste, y_teste)\n",
    "    #performance_history_tree.append(uncertain_sample_score_tree)\n",
    "        \n",
    "    # define tamanho fixo de consultas para aprendizado\n",
    "    n_queries = QUERIES\n",
    "    \n",
    "    #inicio aprendizado ativo\n",
    "    #for index in range(n_amostras_pool):\n",
    "    for index in range(n_queries):\n",
    "        #recupera amostras do pool baseado na estratégia de consulta\n",
    "        query_index, query_instance = committee.query(X_pool)\n",
    "            \n",
    "        # Ensina ao modelo ActiveLearner o registro solicitado (amostras vão para o topo).\n",
    "        committee.teach(X=X_pool[query_index].reshape(1, -1),y=y_pool[query_index].reshape(1, ))\n",
    "       \n",
    "        # apaga os modelos consultados\n",
    "        X_pool = np.delete(X_pool, query_index, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_index)\n",
    "        \n",
    "        # verifica a performance após a inclusão de dados novos \n",
    "        committee_sample_score_tree = committee.score(X_teste, y_teste)\n",
    "        predictions = committee.predict(X_teste)\n",
    "        clear_output(wait=True)\n",
    "        print('Accuracy TREE after query no. %d: %f' % (index+1, committee_sample_score_tree))\n",
    "        arquivo_performance_tree.write('Accuracy after query no. %d: %f \\n' % (index+1,committee_sample_score_tree))\n",
    "        performance_history_tree.append(committee_sample_score_tree)\n",
    "        #print ('Accuracy after query no. %d: %f' % (index+1, accuracy_score(y_test, predictions)))\n",
    "        arquivo_performance_tree.write('Accuracy after query no. %d: %f \\n' % (index+1,accuracy_score(y_teste, predictions)))\n",
    "        #print ('Precision after query no. %d: %f' % (index+1, precision_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "        arquivo_performance_tree.write('Precision after query no. %d: %f \\n' % (index+1,precision_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "        #print ('Recall after query no. %d: %f' % (index+1, recall_score(y_test, predictions, average='macro',zero_division=1)))\n",
    "        arquivo_performance_tree.write('Recall after query no. %d: %f \\n' % (index+1, recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "        #print ('F1 score after query no. %d: %f' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "        f1score= 2*((precision_score(y_teste, predictions,average='macro',zero_division=1)*recall_score(y_teste, predictions, average='macro',zero_division=1))/(precision_score(y_teste, predictions,average='macro',zero_division=1)+recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "        arquivo_performance_tree.write('F1 score after query no. %d: %f \\n' % (index+1, f1score))   \n",
    "        #print (\"========================================\")\n",
    "        arquivo_performance_tree.write('======================================== \\n')\n",
    "        #for i in range(2):\n",
    "            #clear_output(wait=True)\n",
    "            #print(i,\"Aguarde, em execução!\", flush=True)\n",
    "        \n",
    "    \n",
    "    \n",
    "    np.savetxt(arquivo_history_tree, performance_history_tree,delimiter=\",\")\n",
    "    arquivo_performance_tree.close()\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit, train_test_split\n",
    "from timeit import Timer\n",
    "import time\n",
    "import functools\n",
    "# importing the multiprocessing module\n",
    "import multiprocessing\n",
    "\n",
    "#inicia relogio\n",
    "t1 = time.time()\n",
    "# Define o tamanho das divisões feitas no dataset (cross-validation)\n",
    "n_dobras = 10\n",
    "# Define Tamanho inicial da amostra (toda estratégia parte de um tamanho mínimo aleatório).\n",
    "t_inicial = 10\n",
    "#Define contador de amostras utilizadas pela estratégia\n",
    "sample_size = 0 \n",
    "#define array de performance\n",
    "performance_history = []\n",
    "#define array de indices das partições\n",
    "idx_data =[]\n",
    "# cross validation bags - n_splits\n",
    "data_cv = StratifiedShuffleSplit(n_splits= n_dobras,random_state=42) \n",
    "data_cv.get_n_splits(X_raw, y_raw)\n",
    "# chame a instância e gere os dados sobre a base original\n",
    "type(data_cv.split(X_raw, y_raw))\n",
    "# dividir os dados - A função split.split () retorna índices para amostras de treino e amostras de teste. \n",
    "# Ele examinará o número de validação cruzada especificado e retornará cada vez que treinar \n",
    "# e testar os índices de amostra usando os conjuntos de dados de treinamento e teste que podem \n",
    "# ser criados filtrando o conjunto de dados inteiro. Por exemplo idx_data[0][1], o primeiro indice faz referencia\n",
    "# a dobra e o segundo indice faz referencia a posição da dobra (0 = treino e 1 = teste). Logo TRAIN=0 e TEST=1\n",
    "for train_index, test_index in data_cv.split(X_raw,y_raw):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    #print(\"n_split\",n_splits,\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    idx_data.append([train_index, test_index])\n",
    "\n",
    "TRAIN =0\n",
    "TEST =1\n",
    "\n",
    "# define arrays de performance\n",
    "performance_history_knn = []\n",
    "performance_history_rf = []\n",
    "performance_history_tree = []\n",
    "\n",
    "# Define numero de queries\n",
    "QUERIES=50\n",
    "\n",
    "#classificadores=['knn','rf','nb','tree']\n",
    "#classificadores=['knn']\n",
    "classificador_knn=\"knn\"\n",
    "classificador_rf=\"rf\"\n",
    "classificador_nb=\"nb\"\n",
    "classificador_tree=\"tree\"\n",
    "#Multiprocess\n",
    "#for idx_dobra in range(n_dobras):\n",
    "for idx_dobra in range(1):\n",
    "    if __name__ == \"__main__\":\n",
    "        # criando os processos\n",
    "        p1 = multiprocessing.Process(target=committee_sampling_knn(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, classificador_knn, t_inicial))\n",
    "        p2 = multiprocessing.Process(target=committee_sampling_rf(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, classificador_rf, t_inicial))\n",
    "        #p3 = multiprocessing.Process(target=committee_sampling(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, classificador_nb, t_inicial))\n",
    "        p4 = multiprocessing.Process(target=committee_sampling_tree(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, classificador_tree, t_inicial))\n",
    "        \n",
    "        # iniciando os processos\n",
    "        p1.start()\n",
    "        p2.start()\n",
    "        #p3.start()\n",
    "        p4.start()\n",
    "        \n",
    "        # aguardando os processos serem finalizados\n",
    "        p1.join()\n",
    "        p2.join()\n",
    "        #p3.join()\n",
    "        p4.join()\n",
    "        \n",
    "        # todos os processos finalizados\n",
    "        print(\"Terminado!\")\n",
    "\n",
    "#Thread\n",
    "#for idx_dobra in range(1):\n",
    "#    threading.Thread(target=committee_sampling_knn(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, classificador_knn, t_inicial)).start()\n",
    "#    threading.Thread(target=committee_sampling_rf(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, classificador_rf, t_inicial)).start()\n",
    "    #threading.Thread(target=committee_sampling_nb(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, classificador_nb, t_inicial)).start()\n",
    "#    threading.Thread(target=committee_sampling_tree(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, classificador_tree, t_inicial)).start()\n",
    "\n",
    "    \n",
    "t2 = time.time()\n",
    "time_elapsed = (t2 -t1)/3600\n",
    "print(\"tempo: \",time_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot our performance over time.\n",
    "fig, ax = plt.subplots(figsize=(8.5, 6), dpi=130)\n",
    "\n",
    "ax.plot(performance_history_knn,color=\"blue\")\n",
    "ax.plot(performance_history_rf,color=\"red\")\n",
    "ax.plot(performance_history_tree,color=\"green\")\n",
    "ax.scatter(range(len(performance_history_knn)), performance_history_knn,s=0)\n",
    "ax.scatter(range(len(performance_history_rf)), performance_history_rf, s=0)\n",
    "ax.scatter(range(len(performance_history_tree)), performance_history_tree,s=0)\n",
    "\n",
    "ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=5, integer=True))\n",
    "ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=10))\n",
    "ax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=1))\n",
    "\n",
    "ax.set_ylim(bottom=0, top=1)\n",
    "ax.grid(True)\n",
    "\n",
    "ax.set_title('Incremental classification accuracy')\n",
    "ax.set_xlabel('Query iteration')\n",
    "ax.set_ylabel('Classification Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
