{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dst Port</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Tot Fwd Pkts</th>\n",
       "      <th>Tot Bwd Pkts</th>\n",
       "      <th>TotLen Fwd Pkts</th>\n",
       "      <th>TotLen Bwd Pkts</th>\n",
       "      <th>Fwd Pkt Len Max</th>\n",
       "      <th>Fwd Pkt Len Min</th>\n",
       "      <th>Fwd Pkt Len Mean</th>\n",
       "      <th>...</th>\n",
       "      <th>Fwd Seg Size Min</th>\n",
       "      <th>Active Mean</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80</td>\n",
       "      <td>17</td>\n",
       "      <td>119893441</td>\n",
       "      <td>309629</td>\n",
       "      <td>0</td>\n",
       "      <td>9908128</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DDOS attack-LOIC-UDP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>17</td>\n",
       "      <td>119745803</td>\n",
       "      <td>259444</td>\n",
       "      <td>0</td>\n",
       "      <td>8302208</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DDOS attack-LOIC-UDP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80</td>\n",
       "      <td>17</td>\n",
       "      <td>119965025</td>\n",
       "      <td>248800</td>\n",
       "      <td>0</td>\n",
       "      <td>7961600</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DDOS attack-LOIC-UDP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>17</td>\n",
       "      <td>119999991</td>\n",
       "      <td>272337</td>\n",
       "      <td>0</td>\n",
       "      <td>8714784</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DDOS attack-LOIC-UDP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80</td>\n",
       "      <td>17</td>\n",
       "      <td>119820553</td>\n",
       "      <td>219539</td>\n",
       "      <td>0</td>\n",
       "      <td>7025248</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>DDOS attack-LOIC-UDP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3468</th>\n",
       "      <td>3389</td>\n",
       "      <td>6</td>\n",
       "      <td>3868068</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1431</td>\n",
       "      <td>1727</td>\n",
       "      <td>725</td>\n",
       "      <td>0</td>\n",
       "      <td>102.214286</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3469</th>\n",
       "      <td>3389</td>\n",
       "      <td>6</td>\n",
       "      <td>3996691</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>1434</td>\n",
       "      <td>1727</td>\n",
       "      <td>725</td>\n",
       "      <td>0</td>\n",
       "      <td>95.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3470</th>\n",
       "      <td>3389</td>\n",
       "      <td>6</td>\n",
       "      <td>3945545</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1451</td>\n",
       "      <td>1727</td>\n",
       "      <td>741</td>\n",
       "      <td>0</td>\n",
       "      <td>103.642857</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3471</th>\n",
       "      <td>3389</td>\n",
       "      <td>6</td>\n",
       "      <td>3972090</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1432</td>\n",
       "      <td>1727</td>\n",
       "      <td>725</td>\n",
       "      <td>0</td>\n",
       "      <td>102.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3472</th>\n",
       "      <td>3389</td>\n",
       "      <td>6</td>\n",
       "      <td>4035034</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1430</td>\n",
       "      <td>1727</td>\n",
       "      <td>725</td>\n",
       "      <td>0</td>\n",
       "      <td>102.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3473 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dst Port  Protocol  Flow Duration  Tot Fwd Pkts  Tot Bwd Pkts  \\\n",
       "0           80        17      119893441        309629             0   \n",
       "1           80        17      119745803        259444             0   \n",
       "2           80        17      119965025        248800             0   \n",
       "3           80        17      119999991        272337             0   \n",
       "4           80        17      119820553        219539             0   \n",
       "...        ...       ...            ...           ...           ...   \n",
       "3468      3389         6        3868068            14             8   \n",
       "3469      3389         6        3996691            15             8   \n",
       "3470      3389         6        3945545            14             8   \n",
       "3471      3389         6        3972090            14             8   \n",
       "3472      3389         6        4035034            14             8   \n",
       "\n",
       "      TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  Fwd Pkt Len Min  \\\n",
       "0             9908128                0               32               32   \n",
       "1             8302208                0               32               32   \n",
       "2             7961600                0               32               32   \n",
       "3             8714784                0               32               32   \n",
       "4             7025248                0               32               32   \n",
       "...               ...              ...              ...              ...   \n",
       "3468             1431             1727              725                0   \n",
       "3469             1434             1727              725                0   \n",
       "3470             1451             1727              741                0   \n",
       "3471             1432             1727              725                0   \n",
       "3472             1430             1727              725                0   \n",
       "\n",
       "      Fwd Pkt Len Mean  ...  Fwd Seg Size Min  Active Mean  Active Std  \\\n",
       "0            32.000000  ...                 8          0.0         0.0   \n",
       "1            32.000000  ...                 8          0.0         0.0   \n",
       "2            32.000000  ...                 8          0.0         0.0   \n",
       "3            32.000000  ...                 8          0.0         0.0   \n",
       "4            32.000000  ...                 8          0.0         0.0   \n",
       "...                ...  ...               ...          ...         ...   \n",
       "3468        102.214286  ...                20          0.0         0.0   \n",
       "3469         95.600000  ...                20          0.0         0.0   \n",
       "3470        103.642857  ...                20          0.0         0.0   \n",
       "3471        102.285714  ...                20          0.0         0.0   \n",
       "3472        102.142857  ...                20          0.0         0.0   \n",
       "\n",
       "      Active Max  Active Min  Idle Mean  Idle Std  Idle Max  Idle Min  \\\n",
       "0              0           0        0.0       0.0         0         0   \n",
       "1              0           0        0.0       0.0         0         0   \n",
       "2              0           0        0.0       0.0         0         0   \n",
       "3              0           0        0.0       0.0         0         0   \n",
       "4              0           0        0.0       0.0         0         0   \n",
       "...          ...         ...        ...       ...       ...       ...   \n",
       "3468           0           0        0.0       0.0         0         0   \n",
       "3469           0           0        0.0       0.0         0         0   \n",
       "3470           0           0        0.0       0.0         0         0   \n",
       "3471           0           0        0.0       0.0         0         0   \n",
       "3472           0           0        0.0       0.0         0         0   \n",
       "\n",
       "                     Label  \n",
       "0     DDOS attack-LOIC-UDP  \n",
       "1     DDOS attack-LOIC-UDP  \n",
       "2     DDOS attack-LOIC-UDP  \n",
       "3     DDOS attack-LOIC-UDP  \n",
       "4     DDOS attack-LOIC-UDP  \n",
       "...                    ...  \n",
       "3468                Benign  \n",
       "3469                Benign  \n",
       "3470                Benign  \n",
       "3471                Benign  \n",
       "3472                Benign  \n",
       "\n",
       "[3473 rows x 79 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#carrega o dataset\n",
    "dados = pd.read_csv('/home/erasmor/Downloads/2017/todos_apenas_baixa_representatividade.csv',sep=\",\",encoding = 'utf-8',  header=0,na_values='.',dtype={'Label':'category'})\n",
    "\n",
    "#remove valores infinitos\n",
    "dados.replace(-np.Inf, np.nan)\n",
    "\n",
    "#substitui valores NaN\n",
    "dados.fillna(dados.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dst Port            int64\n",
       "Protocol            int64\n",
       "Flow Duration       int64\n",
       "Tot Fwd Pkts        int64\n",
       "Tot Bwd Pkts        int64\n",
       "                   ...   \n",
       "Idle Mean         float64\n",
       "Idle Std          float64\n",
       "Idle Max            int64\n",
       "Idle Min            int64\n",
       "Label            category\n",
       "Length: 79, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index              128\n",
       "Dst Port         27784\n",
       "Protocol         27784\n",
       "Flow Duration    27784\n",
       "Tot Fwd Pkts     27784\n",
       "                 ...  \n",
       "Idle Mean        27784\n",
       "Idle Std         27784\n",
       "Idle Max         27784\n",
       "Idle Min         27784\n",
       "Label             3989\n",
       "Length: 80, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numero de linhas e colunas:  (3473, 79)\n"
     ]
    }
   ],
   "source": [
    "# verifica quantas instâncias (linhas) e quantos atributos (colunas) a base de dados contém\n",
    "print(\"numero de linhas e colunas: \",dados.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "Benign                   815\n",
      "Brute Force -Web         611\n",
      "Brute Force -XSS         230\n",
      "DDOS attack-LOIC-UDP    1730\n",
      "SQL Injection             87\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#visualizar distribições por classes contidas no csv - informar nome da classe alvo\n",
    "print(dados.groupby('Label').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_raw_normalize = MinMaxScaler(X_raw_normalize.reshape(0, 1)).reshape(len(X_raw_normalize))\n",
    "#X_raw_normalizetd2 = (X_raw_normalize - X_raw_normalize.min(axis=0)) / (X_raw_normalize.max(axis=0) - X_raw_normalize.min(axis=0))\n",
    "# Obtendo os nomes das colunas do DataFrame como uma lista.\n",
    "cols = list(dados.columns)\n",
    "# colunas que nao serao normalizadas\n",
    "cols.remove('Label')\n",
    "\n",
    "# Copiando os dados e aplicando a normalizacao por reescala nas colunas do DataFrame que contem\n",
    "# valores continuos. Por padrao, o metodo minmax_scale reescala com min=0 e max=1.\n",
    "dados = dados[~dados.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "dados[cols] = dados[cols].apply(minmax_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define as colunas de atributos e a coluna da classe (de 0 a 72 são atributos e após a 72 é a classe)\n",
    "# \"X_raw\" é features/atributos e \"y_raw\" é target/classe ==> As duas formas abaixo dão certo.\n",
    "#array = dataset.values\n",
    "#X_raw = array[:,0:72]\n",
    "#y_raw = array[:,72]\n",
    "X_raw = dados.iloc[:, :-1].values # atributos\n",
    "y_raw = dados.iloc[:, 78].values # classe de ataques\n",
    "X_raw = np.nan_to_num(X_raw.astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformar a variável Y com valores categóricos das classses de ataques em valores:\n",
    "labelencoder_y = LabelEncoder()\n",
    "y_raw = labelencoder_y.fit_transform(y_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando um PCA. O parametro n_components indica a quantidade de dimensoes que a base\n",
    "# original sera reduzida.\n",
    "pca = PCA(n_components=10, whiten=True,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando o pca na base de dados. O atributo 'values' retorna um numpy.array\n",
    "# de duas dimensões (matriz) contendo apenas os valores numericos do DataFrame.\n",
    "X_raw = pca.fit_transform(X_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy SVM after query no. 5: 0.893064\n",
      "Terminado!\n",
      "tempo:  1631723112.3199975\n"
     ]
    }
   ],
   "source": [
    "def uncertain_sampling_knn(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST,t_inicial):\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "    import functools\n",
    "    from IPython.display import clear_output\n",
    "    from modAL.batch import uncertainty_batch_sampling\n",
    "    from functools import partial\n",
    "        \n",
    "    #define nome de arquivos para salvar\n",
    "    indica_pool=str(idx_dobra)\n",
    "    # recupera as amostras de treino iniciais - a extratificação realizada só serve para tal finalidade.\n",
    "    # No caso força a buscar pelo menos uma amostras de cada rótulo disponível (train_size= len(np.unique(y_raw)).\n",
    "    # Realizar a busca aleatoriamente não garante iniciar com uma instância de cada classe.\n",
    "    X_train_inicial, X_test_inicial, y_train_inicial, y_test_inicial = train_test_split(X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]], train_size= len(np.unique(y_raw[idx_data[idx_dobra][TRAIN]])) + t_inicial, stratify = y_raw[idx_data[idx_dobra][TRAIN]])\n",
    "      \n",
    "    # recupera amostras de teste de acordo com a dobra em uso\n",
    "    X_teste, y_teste = X_raw[idx_data[idx_dobra][TEST]], y_raw[idx_data[idx_dobra][TEST]]\n",
    "    # recupera amostras de treino (será o pool) de acordo com a dobra em uso\n",
    "    X_pool, y_pool = X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]]\n",
    "    #isola exemplos rotulados para o treinamento inicial\n",
    "    X_train = X_train_inicial\n",
    "    y_train = y_train_inicial\n",
    "    #instanciando classificadores de aprendizado ativo\n",
    "    preset_batch = partial(uncertainty_batch_sampling, n_instances=BATCH_SIZE)\n",
    "    learner_knn = ActiveLearner(estimator=KNeighborsClassifier(n_neighbors=5),X_training=X_train, y_training=y_train,query_strategy=preset_batch)\n",
    "    arquivo_performance_knn = open(\"ranked_batch_mode_knn_dobra_\"+indica_pool+\".txt\",\"a\")\n",
    "    arquivo_history_knn = (\"ranked_batch_mode_knn_dobra_\"+indica_pool+\".csv\")\n",
    "    \n",
    "    #Registro da pontuação na porção de teste com o treinamento inicial\n",
    "    uncertain_sample_score_knn = learner_knn.score(X_teste, y_teste)\n",
    "    performance_history_knn.append(uncertain_sample_score_knn)\n",
    "    \n",
    "    #inicio aprendizado ativo\n",
    "     \n",
    "    for index in range(N_QUERIES):\n",
    "        #recupera amostras do pool baseado na estratégia de consulta\n",
    "        query_index, query_instance = learner_knn.query(X_pool)\n",
    "        # Ensina ao modelo ActiveLearner o registro solicitado (amostras vão para o topo).\n",
    "        #learner_knn.teach(X=X_pool[query_index],y=y_pool[query_index])\n",
    "        learner_knn.teach(X=X_pool[query_index].reshape(BATCH_SIZE, -1),y=y_pool[query_index].reshape(BATCH_SIZE, ))\n",
    "        # apaga os modelos consultados\n",
    "        X_pool = np.delete(X_pool, query_index, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_index)\n",
    "        # verifica a performance no conjunto de validação visto que o modelo foi treinado com novos dados\n",
    "        uncertain_sample_score_knn = learner_knn.score(X_teste, y_teste)\n",
    "        predictions = learner_knn.predict(X_teste)\n",
    "        clear_output(wait=True)\n",
    "        print ('Accuracy KNN after query no. %d: %f' % (index+1, uncertain_sample_score_knn))\n",
    "        arquivo_performance_knn.write('Accuracy after query no. %d: %f \\n' % (index+1,uncertain_sample_score_knn))\n",
    "        performance_history_knn.append(uncertain_sample_score_knn)\n",
    "        #print ('Precision after query no. %d: %f' % (index+1, precision_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "        arquivo_performance_knn.write('Precision after query no. %d: %f \\n' % (index+1,precision_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "        #print ('Recall after query no. %d: %f' % (index+1, recall_score(y_test, predictions, average='macro',zero_division=1)))\n",
    "        arquivo_performance_knn.write('Recall after query no. %d: %f \\n' % (index+1, recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "        #print ('F1 score after query no. %d: %f' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "        #arquivo_performance_knn.write('F1 score after query no. %d: %f \\n' % (index+1, f1_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "        f1score= 2*((precision_score(y_teste, predictions,average='macro',zero_division=1)*recall_score(y_teste, predictions, average='macro',zero_division=1))/(precision_score(y_teste, predictions,average='macro',zero_division=1)+recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "        arquivo_performance_knn.write('F1 score after query no. %d: %f \\n' % (index+1, f1score))\n",
    "        #print (\"========================================\")\n",
    "        arquivo_performance_knn.write('======================================== \\n')\n",
    "                       \n",
    "        \n",
    "    arquivo_performance_knn.write(\"\\n Avaliação final KNN \\n\")\n",
    "    arquivo_performance_knn.write(classification_report(y_teste, predictions,zero_division=1))   \n",
    "    np.savetxt(arquivo_history_knn, performance_history_knn,delimiter=\",\")\n",
    "    arquivo_performance_knn.close()\n",
    "\n",
    "def uncertain_sampling_rf(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST,t_inicial):\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "    import functools\n",
    "    from IPython.display import clear_output\n",
    "    from modAL.batch import uncertainty_batch_sampling\n",
    "    from functools import partial\n",
    "        \n",
    "    #define nome de arquivos para salvar\n",
    "    indica_pool=str(idx_dobra)\n",
    "    # recupera as amostras de treino iniciais - a extratificação realizada só serve para tal finalidade.\n",
    "    # No caso força a buscar pelo menos uma amostras de cada rótulo disponível (train_size= len(np.unique(y_raw)).\n",
    "    # Realizar a busca aleatoriamente não garante iniciar com uma instância de cada classe.\n",
    "    X_train_inicial, X_test_inicial, y_train_inicial, y_test_inicial = train_test_split(X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]], train_size= len(np.unique(y_raw[idx_data[idx_dobra][TRAIN]])) + t_inicial, stratify = y_raw[idx_data[idx_dobra][TRAIN]])\n",
    "      \n",
    "    # recupera amostras de teste de acordo com a dobra em uso\n",
    "    X_teste, y_teste = X_raw[idx_data[idx_dobra][TEST]], y_raw[idx_data[idx_dobra][TEST]]\n",
    "    # recupera amostras de treino (será o pool) de acordo com a dobra em uso\n",
    "    X_pool, y_pool = X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]]\n",
    "    #isola exemplos rotulados para o treinamento inicial\n",
    "    X_train = X_train_inicial\n",
    "    y_train = y_train_inicial\n",
    "    #instanciando classificadores de aprendizado ativo\n",
    "    preset_batch = partial(uncertainty_batch_sampling, n_instances=BATCH_SIZE)\n",
    "    learner_rf = ActiveLearner(estimator=RandomForestClassifier(random_state=42, n_estimators= 5, max_depth=5),X_training=X_train, y_training=y_train,query_strategy=preset_batch)\n",
    "    arquivo_performance_rf = open(\"ranked_batch_mode_rf_dobra_\"+indica_pool+\".txt\",\"a\")\n",
    "    arquivo_history_rf = (\"ranked_batch_mode_rf_dobra_\"+indica_pool+\".csv\")\n",
    "    #Registro da pontuação na porção de teste com o treinamento inicial\n",
    "    uncertain_sample_score_rf = learner_rf.score(X_teste, y_teste)\n",
    "    performance_history_rf.append(uncertain_sample_score_rf)\n",
    "    \n",
    "    #inicio aprendizado ativo\n",
    "     \n",
    "    for index in range(N_QUERIES):\n",
    "        #recupera amostras do pool baseado na estratégia de consulta\n",
    "        query_index, query_instance = learner_rf.query(X_pool)\n",
    "        # Ensina ao modelo ActiveLearner o registro solicitado (amostras vão para o topo).\n",
    "        #learner_rf.teach(X=X_pool[query_index],y=y_pool[query_index])\n",
    "        learner_rf.teach(X=X_pool[query_index].reshape(BATCH_SIZE, -1),y=y_pool[query_index].reshape(BATCH_SIZE, ))\n",
    "        # apaga os modelos consultados\n",
    "        X_pool = np.delete(X_pool, query_index, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_index)\n",
    "        # verifica a performance no conjunto de validação visto que o modelo foi treinado com novos dados\n",
    "        uncertain_sample_score_rf = learner_rf.score(X_teste, y_teste)\n",
    "        predictions = learner_rf.predict(X_teste)\n",
    "        clear_output(wait=True)\n",
    "        print ('Accuracy RF after query no. %d: %f' % (index+1, uncertain_sample_score_rf))\n",
    "        arquivo_performance_rf.write('Accuracy after query no. %d: %f \\n' % (index+1,uncertain_sample_score_rf))\n",
    "        performance_history_rf.append(uncertain_sample_score_rf)\n",
    "        #print ('Precision after query no. %d: %f' % (index+1, precision_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "        arquivo_performance_rf.write('Precision after query no. %d: %f \\n' % (index+1,precision_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "        #print ('Recall after query no. %d: %f' % (index+1, recall_score(y_test, predictions, average='macro',zero_division=1)))\n",
    "        arquivo_performance_rf.write('Recall after query no. %d: %f \\n' % (index+1, recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "        #print ('F1 score after query no. %d: %f' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "        #arquivo_performance_rf.write('F1 score after query no. %d: %f \\n' % (index+1, f1_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "        f1score= 2*((precision_score(y_teste, predictions,average='macro',zero_division=1)*recall_score(y_teste, predictions, average='macro',zero_division=1))/(precision_score(y_teste, predictions,average='macro',zero_division=1)+recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "        arquivo_performance_rf.write('F1 score after query no. %d: %f \\n' % (index+1, f1score))\n",
    "        #print (\"========================================\")\n",
    "        arquivo_performance_rf.write('======================================== \\n')\n",
    "                     \n",
    "        \n",
    "    arquivo_performance_rf.write(\"\\n Avaliação final RF \\n\")\n",
    "    arquivo_performance_rf.write(classification_report(y_teste, predictions,zero_division=1))  \n",
    "    np.savetxt(arquivo_history_rf, performance_history_rf,delimiter=\",\")\n",
    "    arquivo_performance_rf.close()\n",
    "\n",
    "def uncertain_sampling_tree(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, t_inicial):\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "    import functools\n",
    "    from IPython.display import clear_output\n",
    "    from modAL.batch import uncertainty_batch_sampling\n",
    "    from functools import partial\n",
    "        \n",
    "    #define nome de arquivos para salvar\n",
    "    indica_pool=str(idx_dobra)\n",
    "    # recupera as amostras de treino iniciais - a extratificação realizada só serve para tal finalidade.\n",
    "    # No caso força a buscar pelo menos uma amostras de cada rótulo disponível (train_size= len(np.unique(y_raw)).\n",
    "    # Realizar a busca aleatoriamente não garante iniciar com uma instância de cada classe.\n",
    "    X_train_inicial, X_test_inicial, y_train_inicial, y_test_inicial = train_test_split(X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]], train_size= len(np.unique(y_raw[idx_data[idx_dobra][TRAIN]])) + t_inicial, stratify = y_raw[idx_data[idx_dobra][TRAIN]])\n",
    "      \n",
    "    # recupera amostras de teste de acordo com a dobra em uso\n",
    "    X_teste, y_teste = X_raw[idx_data[idx_dobra][TEST]], y_raw[idx_data[idx_dobra][TEST]]\n",
    "    # recupera amostras de treino (será o pool) de acordo com a dobra em uso\n",
    "    X_pool, y_pool = X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]]\n",
    "    #isola exemplos rotulados para o treinamento inicial\n",
    "    X_train = X_train_inicial\n",
    "    y_train = y_train_inicial\n",
    "    #instanciando classificadores de aprendizado ativo\n",
    "    preset_batch = partial(uncertainty_batch_sampling, n_instances=BATCH_SIZE)\n",
    "    learner_tree = ActiveLearner(estimator=DecisionTreeClassifier(max_depth=5,min_samples_split=2,min_samples_leaf=2),X_training=X_train, y_training=y_train,query_strategy=preset_batch)\n",
    "    arquivo_performance_tree = open(\"ranked_batch_mode_tree_dobra_\"+indica_pool+\".txt\",\"a\")\n",
    "    arquivo_history_tree = (\"ranked_batch_mode_tree_dobra_\"+indica_pool+\".csv\")\n",
    "    #Registro da pontuação na porção de teste com o treinamento inicial\n",
    "    uncertain_sample_score_tree = learner_tree.score(X_teste, y_teste)\n",
    "    performance_history_tree.append(uncertain_sample_score_tree)\n",
    "      \n",
    "    #inicio aprendizado ativo\n",
    "     \n",
    "    for index in range(N_QUERIES):\n",
    "        #recupera amostras do pool baseado na estratégia de consulta\n",
    "        query_index, query_instance = learner_tree.query(X_pool)\n",
    "        # Ensina ao modelo ActiveLearner o registro solicitado (amostras vão para o topo).\n",
    "        #learner_tree.teach(X=X_pool[query_index],y=y_pool[query_index])\n",
    "        learner_tree.teach(X=X_pool[query_index].reshape(BATCH_SIZE, -1),y=y_pool[query_index].reshape(BATCH_SIZE, ))\n",
    "        # apaga os modelos consultados\n",
    "        X_pool = np.delete(X_pool, query_index, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_index)\n",
    "        \n",
    "        # verifica a performance no conjunto de validação visto que o modelo foi treinado com novos dados\n",
    "        uncertain_sample_score_tree = learner_tree.score(X_teste, y_teste)\n",
    "        predictions = learner_tree.predict(X_teste)\n",
    "        clear_output(wait=True)\n",
    "        print ('Accuracy TREE after query no. %d: %f' % (index+1, uncertain_sample_score_tree))\n",
    "        arquivo_performance_tree.write('Accuracy after query no. %d: %f \\n' % (index+1,uncertain_sample_score_tree))\n",
    "        performance_history_tree.append(uncertain_sample_score_tree)\n",
    "        #print ('Precision after query no. %d: %f' % (index+1, precision_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "        arquivo_performance_tree.write('Precision after query no. %d: %f \\n' % (index+1,precision_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "        #print ('Recall after query no. %d: %f' % (index+1, recall_score(y_test, predictions, average='macro',zero_division=1)))\n",
    "        arquivo_performance_tree.write('Recall after query no. %d: %f \\n' % (index+1, recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "        #print ('F1 score after query no. %d: %f' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "        #arquivo_performance_tree.write('F1 score after query no. %d: %f \\n' % (index+1, f1_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "        f1score= 2*((precision_score(y_teste, predictions,average='macro',zero_division=1)*recall_score(y_teste, predictions, average='macro',zero_division=1))/(precision_score(y_teste, predictions,average='macro',zero_division=1)+recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "        arquivo_performance_tree.write('F1 score after query no. %d: %f \\n' % (index+1, f1score))\n",
    "        #print (\"========================================\")\n",
    "        arquivo_performance_tree.write('======================================== \\n')\n",
    "                        \n",
    "        \n",
    "    arquivo_performance_tree.write(\"\\n Avaliação final TREE \\n\")\n",
    "    arquivo_performance_tree.write(classification_report(y_teste, predictions,zero_division=1))  \n",
    "    np.savetxt(arquivo_history_tree, performance_history_tree,delimiter=\",\")\n",
    "    arquivo_performance_tree.close()\n",
    "     \n",
    "def uncertain_sampling_mlp(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, t_inicial):\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "    import functools\n",
    "    from IPython.display import clear_output\n",
    "    from modAL.batch import uncertainty_batch_sampling\n",
    "    from functools import partial\n",
    "        \n",
    "    #define nome de arquivos para salvar\n",
    "    indica_pool=str(idx_dobra)\n",
    "    # recupera as amostras de treino iniciais - a extratificação realizada só serve para tal finalidade.\n",
    "    # No caso força a buscar pelo menos uma amostras de cada rótulo disponível (train_size= len(np.unique(y_raw)).\n",
    "    # Realizar a busca aleatoriamente não garante iniciar com uma instância de cada classe.\n",
    "    X_train_inicial, X_test_inicial, y_train_inicial, y_test_inicial = train_test_split(X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]], train_size= len(np.unique(y_raw[idx_data[idx_dobra][TRAIN]])) + t_inicial, stratify = y_raw[idx_data[idx_dobra][TRAIN]])\n",
    "      \n",
    "    # recupera amostras de teste de acordo com a dobra em uso\n",
    "    X_teste, y_teste = X_raw[idx_data[idx_dobra][TEST]], y_raw[idx_data[idx_dobra][TEST]]\n",
    "    # recupera amostras de treino (será o pool) de acordo com a dobra em uso\n",
    "    X_pool, y_pool = X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]]\n",
    "    #isola exemplos rotulados para o treinamento inicial\n",
    "    X_train = X_train_inicial\n",
    "    y_train = y_train_inicial\n",
    "    #instanciando classificadores de aprendizado ativo\n",
    "    preset_batch = partial(uncertainty_batch_sampling, n_instances=BATCH_SIZE)\n",
    "    learner_mlp = ActiveLearner(estimator=MLPClassifier(max_iter = 2000),X_training=X_train, y_training=y_train,query_strategy=preset_batch)\n",
    "    arquivo_performance_mlp = open(\"ranked_batch_mode_mlp_dobra_\"+indica_pool+\".txt\",\"a\")\n",
    "    arquivo_history_mlp = (\"ranked_batch_mode_mlp_dobra_\"+indica_pool+\".csv\")\n",
    "    #Registro da pontuação na porção de teste com o treinamento inicial\n",
    "    uncertain_sample_score_mlp = learner_mlp.score(X_teste, y_teste)\n",
    "    performance_history_mlp.append(uncertain_sample_score_mlp)\n",
    "      \n",
    "    #inicio aprendizado ativo\n",
    "     \n",
    "    for index in range(N_QUERIES):\n",
    "        #recupera amostras do pool baseado na estratégia de consulta\n",
    "        query_index, query_instance = learner_mlp.query(X_pool)\n",
    "        # Ensina ao modelo ActiveLearner o registro solicitado (amostras vão para o topo).\n",
    "        #learner_mlp.teach(X=X_pool[query_index],y=y_pool[query_index])\n",
    "        learner_mlp.teach(X=X_pool[query_index].reshape(BATCH_SIZE, -1),y=y_pool[query_index].reshape(BATCH_SIZE, ))\n",
    "        # apaga os modelos consultados\n",
    "        X_pool = np.delete(X_pool, query_index, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_index)\n",
    "        # verifica a performance no conjunto de validação visto que o modelo foi treinado com novos dados\n",
    "        uncertain_sample_score_mlp = learner_mlp.score(X_teste, y_teste)\n",
    "        predictions = learner_mlp.predict(X_teste)\n",
    "        clear_output(wait=True)\n",
    "        print ('Accuracy MLP after query no. %d: %f' % (index+1, uncertain_sample_score_mlp))\n",
    "        arquivo_performance_mlp.write('Accuracy after query no. %d: %f \\n' % (index+1,uncertain_sample_score_mlp))\n",
    "        performance_history_mlp.append(uncertain_sample_score_mlp)\n",
    "        #print ('Precision after query no. %d: %f' % (index+1, precision_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "        arquivo_performance_mlp.write('Precision after query no. %d: %f \\n' % (index+1,precision_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "        #print ('Recall after query no. %d: %f' % (index+1, recall_score(y_test, predictions, average='macro',zero_division=1)))\n",
    "        arquivo_performance_mlp.write('Recall after query no. %d: %f \\n' % (index+1, recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "        #print ('F1 score after query no. %d: %f' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "        #arquivo_performance_mlp.write('F1 score after query no. %d: %f \\n' % (index+1, f1_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "        f1score= 2*((precision_score(y_teste, predictions,average='macro',zero_division=1)*recall_score(y_teste, predictions, average='macro',zero_division=1))/(precision_score(y_teste, predictions,average='macro',zero_division=1)+recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "        arquivo_performance_mlp.write('F1 score after query no. %d: %f \\n' % (index+1, f1score))\n",
    "        #print (\"========================================\")\n",
    "        arquivo_performance_mlp.write('======================================== \\n')\n",
    "                        \n",
    "        \n",
    "    arquivo_performance_mlp.write(\"\\n Avaliação final MLP \\n\")\n",
    "    arquivo_performance_mlp.write(classification_report(y_teste, predictions,zero_division=1))  \n",
    "    np.savetxt(arquivo_history_mlp, performance_history_mlp,delimiter=\",\")\n",
    "    arquivo_performance_mlp.close()\n",
    "\n",
    "def uncertain_sampling_xgb(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, t_inicial):\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "    import functools\n",
    "    from IPython.display import clear_output\n",
    "    from modAL.batch import uncertainty_batch_sampling\n",
    "    from functools import partial\n",
    "        \n",
    "    #define nome de arquivos para salvar\n",
    "    indica_pool=str(idx_dobra)\n",
    "    # recupera as amostras de treino iniciais - a extratificação realizada só serve para tal finalidade.\n",
    "    # No caso força a buscar pelo menos uma amostras de cada rótulo disponível (train_size= len(np.unique(y_raw)).\n",
    "    # Realizar a busca aleatoriamente não garante iniciar com uma instância de cada classe.\n",
    "    X_train_inicial, X_test_inicial, y_train_inicial, y_test_inicial = train_test_split(X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]], train_size= len(np.unique(y_raw[idx_data[idx_dobra][TRAIN]])) + t_inicial, stratify = y_raw[idx_data[idx_dobra][TRAIN]])\n",
    "    # recupera amostras de teste de acordo com a dobra em uso\n",
    "    X_teste, y_teste = X_raw[idx_data[idx_dobra][TEST]], y_raw[idx_data[idx_dobra][TEST]]\n",
    "    # recupera amostras de treino (será o pool) de acordo com a dobra em uso\n",
    "    X_pool, y_pool = X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]]\n",
    "    #isola exemplos rotulados para o treinamento inicial\n",
    "    X_train = X_train_inicial\n",
    "    y_train = y_train_inicial\n",
    "    #instanciando classificadores de aprendizado ativo\n",
    "    preset_batch = partial(uncertainty_batch_sampling, n_instances=BATCH_SIZE)\n",
    "    learner_xgb = ActiveLearner(estimator=GradientBoostingClassifier(n_estimators=7, learning_rate=1.0,max_depth=1, random_state=42),X_training=X_train, y_training=y_train,query_strategy=preset_batch)\n",
    "    arquivo_performance_xgb = open(\"ranked_batch_mode_xgb_dobra_\"+indica_pool+\".txt\",\"a\")\n",
    "    arquivo_history_xgb = (\"ranked_batch_mode_xgb_dobra_\"+indica_pool+\".csv\")\n",
    "    #Registro da pontuação na porção de teste com o treinamento inicial\n",
    "    uncertain_sample_score_xgb = learner_xgb.score(X_teste, y_teste)\n",
    "    performance_history_xgb.append(uncertain_sample_score_xgb)\n",
    "      \n",
    "    #inicio aprendizado ativo\n",
    "     \n",
    "    for index in range(N_QUERIES):\n",
    "        #recupera amostras do pool baseado na estratégia de consulta\n",
    "        query_index, query_instance = learner_xgb.query(X_pool)\n",
    "        \n",
    "        # Ensina ao modelo ActiveLearner o registro solicitado (amostras vão para o topo).\n",
    "        #learner_xgb.teach(X=X_pool[query_index],y=y_pool[query_index])\n",
    "        learner_xgb.teach(X=X_pool[query_index].reshape(BATCH_SIZE, -1),y=y_pool[query_index].reshape(BATCH_SIZE, ))\n",
    "        # apaga os modelos consultados\n",
    "        X_pool = np.delete(X_pool, query_index, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_index)\n",
    "        #print(\"tamanho de X_train - diminuindo: \",X_train.shape)\n",
    "        # verifica a performance no conjunto de validação visto que o modelo foi treinado com novos dados\n",
    "        uncertain_sample_score_xgb = learner_xgb.score(X_teste, y_teste)\n",
    "        predictions = learner_xgb.predict(X_teste)\n",
    "        clear_output(wait=True)\n",
    "        print ('Accuracy XGB after query no. %d: %f' % (index+1, uncertain_sample_score_xgb))\n",
    "        arquivo_performance_xgb.write('Accuracy after query no. %d: %f \\n' % (index+1,uncertain_sample_score_xgb))\n",
    "        performance_history_xgb.append(uncertain_sample_score_xgb)\n",
    "        #print ('Precision after query no. %d: %f' % (index+1, precision_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "        arquivo_performance_xgb.write('Precision after query no. %d: %f \\n' % (index+1,precision_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "        #print ('Recall after query no. %d: %f' % (index+1, recall_score(y_test, predictions, average='macro',zero_division=1)))\n",
    "        arquivo_performance_xgb.write('Recall after query no. %d: %f \\n' % (index+1, recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "        #print ('F1 score after query no. %d: %f' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "        #arquivo_performance_xgb.write('F1 score after query no. %d: %f \\n' % (index+1, f1_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "        f1score= 2*((precision_score(y_teste, predictions,average='macro',zero_division=1)*recall_score(y_teste, predictions, average='macro',zero_division=1))/(precision_score(y_teste, predictions,average='macro',zero_division=1)+recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "        arquivo_performance_xgb.write('F1 score after query no. %d: %f \\n' % (index+1, f1score))\n",
    "        #print (\"========================================\")\n",
    "        arquivo_performance_xgb.write('======================================== \\n')\n",
    "                   \n",
    "        \n",
    "    arquivo_performance_xgb.write(\"\\n Avaliação final XGB \\n\")\n",
    "    arquivo_performance_xgb.write(classification_report(y_teste, predictions,zero_division=1))  \n",
    "    np.savetxt(arquivo_history_xgb, performance_history_xgb,delimiter=\",\")\n",
    "    arquivo_performance_xgb.close()\n",
    "\n",
    "def uncertain_sampling_svm(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, t_inicial):\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn import svm\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "    import functools\n",
    "    from IPython.display import clear_output\n",
    "    from modAL.batch import uncertainty_batch_sampling\n",
    "    from functools import partial\n",
    "        \n",
    "    #define nome de arquivos para salvar\n",
    "    indica_pool=str(idx_dobra)\n",
    "    # recupera as amostras de treino iniciais - a extratificação realizada só serve para tal finalidade.\n",
    "    # No caso força a buscar pelo menos uma amostras de cada rótulo disponível (train_size= len(np.unique(y_raw)).\n",
    "    # Realizar a busca aleatoriamente não garante iniciar com uma instância de cada classe.\n",
    "    X_train_inicial, X_test_inicial, y_train_inicial, y_test_inicial = train_test_split(X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]], train_size= len(np.unique(y_raw[idx_data[idx_dobra][TRAIN]])) + t_inicial, stratify = y_raw[idx_data[idx_dobra][TRAIN]])\n",
    "    # recupera amostras de teste de acordo com a dobra em uso\n",
    "    X_teste, y_teste = X_raw[idx_data[idx_dobra][TEST]], y_raw[idx_data[idx_dobra][TEST]]\n",
    "    # recupera amostras de treino (será o pool) de acordo com a dobra em uso\n",
    "    X_pool, y_pool = X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]]\n",
    "    #isola exemplos rotulados para o treinamento inicial\n",
    "    X_train = X_train_inicial\n",
    "    y_train = y_train_inicial\n",
    "  \n",
    "    #instanciando classificadores de aprendizado ativo\n",
    "    preset_batch = partial(uncertainty_batch_sampling, n_instances=BATCH_SIZE)\n",
    "    learner_svm = ActiveLearner(estimator=svm.SVC(kernel='linear',probability=True),X_training=X_train, y_training=y_train,query_strategy=preset_batch)\n",
    "    arquivo_performance_svm = open(\"ranked_batch_mode_svm_dobra_\"+indica_pool+\".txt\",\"a\")\n",
    "    arquivo_history_svm = (\"ranked_batch_mode_svm_dobra_\"+indica_pool+\".csv\")\n",
    "    #Registro da pontuação na porção de teste com o treinamento inicial\n",
    "    uncertain_sample_score_svm = learner_svm.score(X_teste, y_teste)\n",
    "    performance_history_svm.append(uncertain_sample_score_svm)\n",
    "      \n",
    "    #inicio aprendizado ativo\n",
    "     \n",
    "    for index in range(N_QUERIES):\n",
    "        #recupera amostras do pool baseado na estratégia de consulta\n",
    "        query_index, query_instance = learner_svm.query(X_pool)\n",
    "        # Ensina ao modelo ActiveLearner o registro solicitado (amostras vão para o topo).\n",
    "        #learner_svm.teach(X=X_pool[query_index],y=y_pool[query_index])\n",
    "        learner_svm.teach(X=X_pool[query_index].reshape(BATCH_SIZE, -1),y=y_pool[query_index].reshape(BATCH_SIZE, ))\n",
    "        # apaga os modelos consultados\n",
    "        X_pool = np.delete(X_pool, query_index, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_index)\n",
    "        # verifica a performance no conjunto de validação visto que o modelo foi treinado com novos dados\n",
    "        uncertain_sample_score_svm = learner_svm.score(X_teste, y_teste)\n",
    "        predictions = learner_svm.predict(X_teste)\n",
    "        clear_output(wait=True)\n",
    "        print ('Accuracy SVM after query no. %d: %f' % (index+1, uncertain_sample_score_svm))\n",
    "        arquivo_performance_svm.write('Accuracy after query no. %d: %f \\n' % (index+1,uncertain_sample_score_svm))\n",
    "        performance_history_svm.append(uncertain_sample_score_svm)\n",
    "        #print ('Precision after query no. %d: %f' % (index+1, precision_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "        arquivo_performance_svm.write('Precision after query no. %d: %f \\n' % (index+1,precision_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "        #print ('Recall after query no. %d: %f' % (index+1, recall_score(y_test, predictions, average='macro',zero_division=1)))\n",
    "        arquivo_performance_svm.write('Recall after query no. %d: %f \\n' % (index+1, recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "        #print ('F1 score after query no. %d: %f' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "        #arquivo_performance_svm.write('F1 score after query no. %d: %f \\n' % (index+1, f1_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "        f1score= 2*((precision_score(y_teste, predictions,average='macro',zero_division=1)*recall_score(y_teste, predictions, average='macro',zero_division=1))/(precision_score(y_teste, predictions,average='macro',zero_division=1)+recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "        arquivo_performance_svm.write('F1 score after query no. %d: %f \\n' % (index+1, f1score))\n",
    "        #print (\"========================================\")\n",
    "        arquivo_performance_svm.write('======================================== \\n')\n",
    "                        \n",
    "        \n",
    "    arquivo_performance_svm.write(\"\\n Avaliação final SVM \\n\")\n",
    "    arquivo_performance_svm.write(classification_report(y_teste, predictions,zero_division=1))  \n",
    "    np.savetxt(arquivo_history_svm, performance_history_svm,delimiter=\",\")\n",
    "    arquivo_performance_svm.close()\n",
    "\n",
    "def uncertain_sampling_nb(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, t_inicial):\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn import svm\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "    import functools\n",
    "    from IPython.display import clear_output\n",
    "    from modAL.batch import uncertainty_batch_sampling\n",
    "    from functools import partial\n",
    "        \n",
    "    #define nome de arquivos para salvar\n",
    "    indica_pool=str(idx_dobra)\n",
    "    # recupera as amostras de treino iniciais - a extratificação realizada só serve para tal finalidade.\n",
    "    # No caso força a buscar pelo menos uma amostras de cada rótulo disponível (train_size= len(np.unique(y_raw)).\n",
    "    # Realizar a busca aleatoriamente não garante iniciar com uma instância de cada classe.\n",
    "    X_train_inicial, X_test_inicial, y_train_inicial, y_test_inicial = train_test_split(X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]], train_size= len(np.unique(y_raw[idx_data[idx_dobra][TRAIN]])) + t_inicial, stratify = y_raw[idx_data[idx_dobra][TRAIN]])\n",
    "    # recupera amostras de teste de acordo com a dobra em uso\n",
    "    X_teste, y_teste = X_raw[idx_data[idx_dobra][TEST]], y_raw[idx_data[idx_dobra][TEST]]\n",
    "    # recupera amostras de treino (será o pool) de acordo com a dobra em uso\n",
    "    X_pool, y_pool = X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]]\n",
    "    #isola exemplos rotulados para o treinamento inicial\n",
    "    X_train = X_train_inicial\n",
    "    y_train = y_train_inicial\n",
    "  \n",
    "    #instanciando classificadores de aprendizado ativo\n",
    "    preset_batch = partial(uncertainty_batch_sampling, n_instances=BATCH_SIZE)\n",
    "    learner_nb = ActiveLearner(estimator=GaussianNB(),X_training=X_train, y_training=y_train,query_strategy=preset_batch)\n",
    "    arquivo_performance_nb = open(\"ranked_batch_mode_nb_dobra_\"+indica_pool+\".txt\",\"a\")\n",
    "    arquivo_history_nb = (\"ranked_batch_mode_nb_dobra_\"+indica_pool+\".csv\")\n",
    "    \n",
    "    #Registro da pontuação na porção de teste com o treinamento inicial\n",
    "    uncertain_sample_score_nb = learner_nb.score(X_teste, y_teste)\n",
    "    performance_history_nb.append(uncertain_sample_score_nb)\n",
    "      \n",
    "    #inicio aprendizado ativo\n",
    "     \n",
    "    for index in range(N_QUERIES):\n",
    "        #recupera amostras do pool baseado na estratégia de consulta\n",
    "        query_index, query_instance = learner_nb.query(X_pool)\n",
    "        # Ensina ao modelo ActiveLearner o registro solicitado (amostras vão para o topo).\n",
    "        #learner_nb.teach(X=X_pool[query_index],y=y_pool[query_index])\n",
    "        learner_nb.teach(X=X_pool[query_index].reshape(BATCH_SIZE, -1),y=y_pool[query_index].reshape(BATCH_SIZE, ))\n",
    "        # apaga os modelos consultados\n",
    "        X_pool = np.delete(X_pool, query_index, axis=0)\n",
    "        y_pool = np.delete(y_pool, query_index)\n",
    "        # verifica a performance no conjunto de validação visto que o modelo foi treinado com novos dados\n",
    "        uncertain_sample_score_nb = learner_nb.score(X_teste, y_teste)\n",
    "        predictions = learner_nb.predict(X_teste)\n",
    "        clear_output(wait=True)\n",
    "        print ('Accuracy NB after query no. %d: %f' % (index+1, uncertain_sample_score_nb))\n",
    "        arquivo_performance_nb.write('Accuracy after query no. %d: %f \\n' % (index+1,uncertain_sample_score_nb))\n",
    "        performance_history_nb.append(uncertain_sample_score_nb)\n",
    "        #print ('Precision after query no. %d: %f' % (index+1, precision_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "        arquivo_performance_nb.write('Precision after query no. %d: %f \\n' % (index+1,precision_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "        #print ('Recall after query no. %d: %f' % (index+1, recall_score(y_test, predictions, average='macro',zero_division=1)))\n",
    "        arquivo_performance_nb.write('Recall after query no. %d: %f \\n' % (index+1, recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "        #print ('F1 score after query no. %d: %f' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "        #arquivo_performance_nb.write('F1 score after query no. %d: %f \\n' % (index+1, f1_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "        f1score= 2*((precision_score(y_teste, predictions,average='macro',zero_division=1)*recall_score(y_teste, predictions, average='macro',zero_division=1))/(precision_score(y_teste, predictions,average='macro',zero_division=1)+recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "        arquivo_performance_nb.write('F1 score after query no. %d: %f \\n' % (index+1, f1score))\n",
    "        #print (\"========================================\")\n",
    "        arquivo_performance_nb.write('======================================== \\n')\n",
    "                    \n",
    "        \n",
    "    arquivo_performance_nb.write(\"\\n Avaliação final NB \\n\")\n",
    "    arquivo_performance_nb.write(classification_report(y_teste, predictions,zero_division=1))  \n",
    "    np.savetxt(arquivo_history_nb, performance_history_nb,delimiter=\",\")\n",
    "    arquivo_performance_nb.close()\n",
    "\n",
    "\n",
    "import time\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import threading\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit, train_test_split\n",
    "from modAL.uncertainty import classifier_uncertainty\n",
    "from modAL.models import ActiveLearner\n",
    "#from modAL.uncertainty import uncertainty_sampling\n",
    "from timeit import Timer\n",
    "import time\n",
    "import functools\n",
    "# importing the multiprocessing module\n",
    "import multiprocessing\n",
    "\n",
    "#inicia relogio\n",
    "t1 = time.time()\n",
    "# Define o tamanho das divisões feitas no dataset (cross-validation)\n",
    "n_dobras = 10\n",
    "# Define Tamanho inicial da amostra (toda estratégia parte de um tamanho mínimo aleatório).\n",
    "t_inicial = 10\n",
    "\n",
    "#define array de indices das partições\n",
    "idx_data =[]\n",
    "# cross validation bags - n_splits\n",
    "data_cv = StratifiedShuffleSplit(n_splits= n_dobras, train_size=346, test_size=346, random_state=42) \n",
    "data_cv.get_n_splits(X_raw, y_raw)\n",
    "# chame a instância e gere os dados sobre a base original\n",
    "type(data_cv.split(X_raw, y_raw))\n",
    "# dividir os dados - A função split.split () retorna índices para amostras de treino e amostras de teste. \n",
    "# Ele examinará o número de validação cruzada especificado e retornará cada vez que treinar \n",
    "# e testar os índices de amostra usando os conjuntos de dados de treinamento e teste que podem \n",
    "# ser criados filtrando o conjunto de dados inteiro. Por exemplo idx_data[0][1], o primeiro indice faz referencia\n",
    "# a dobra e o segundo indice faz referencia a posição da dobra (0 = treino e 1 = teste). Logo TRAIN=0 e TEST=1.\n",
    "for train_index, test_index in data_cv.split(X_raw,y_raw):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    #print(\"n_split\",n_splits,\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    idx_data.append([train_index, test_index])\n",
    "#verifica tamanho das dobras (numero de instâncias de cada dobra)\n",
    "#print(\"tamanho de cada dobra de treino: \",idx_data[1][0].shape,\" tamanho de cada dobra de teste: \",idx_data[1][1].shape)\n",
    "\n",
    "TRAIN =0\n",
    "TEST =1\n",
    "\n",
    "performance_history_knn = []\n",
    "performance_history_rf = []\n",
    "performance_history_tree = []\n",
    "performance_history_mlp = []\n",
    "performance_history_xgb = []\n",
    "performance_history_svm = []\n",
    "performance_history_nb = []\n",
    "#define numero de queries\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "N_RAW_SAMPLES = 20\n",
    "N_QUERIES = N_RAW_SAMPLES // BATCH_SIZE\n",
    "\n",
    "#chama procedimento de aprendizado para todas as dobras\n",
    "\n",
    "for idx_dobra in range(1):\n",
    "    if __name__ == \"__main__\":\n",
    "        # criando os processos\n",
    "        p1 = multiprocessing.Process(target=threading.Thread(target=uncertain_sampling_knn(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST,t_inicial)).start())\n",
    "        p2 = multiprocessing.Process(target=threading.Thread(target=uncertain_sampling_rf(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST,t_inicial)).start())\n",
    "        p3 = multiprocessing.Process(target=threading.Thread(target=uncertain_sampling_nb(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST,t_inicial)).start())\n",
    "        p4 = multiprocessing.Process(target=threading.Thread(target=uncertain_sampling_tree(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST,t_inicial)).start())                                                         \n",
    "        p5 = multiprocessing.Process(target=threading.Thread(target=uncertain_sampling_mlp(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST,t_inicial)).start())                                                         \n",
    "        p6 = multiprocessing.Process(target=threading.Thread(target=uncertain_sampling_xgb(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST,t_inicial)).start())                                                         \n",
    "        p7 = multiprocessing.Process(target=threading.Thread(target=uncertain_sampling_svm(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST,t_inicial)).start())                                                         \n",
    "    \n",
    "        # iniciando os processos\n",
    "        \n",
    "        p1.start()\n",
    "        p2.start()\n",
    "        p3.start()\n",
    "        p4.start()\n",
    "        p5.start()\n",
    "        p6.start()\n",
    "        p7.start()\n",
    "        \n",
    "        # aguardando os processos serem finalizados\n",
    "        \n",
    "        p1.join()\n",
    "        p2.join()\n",
    "        p3.join()\n",
    "        p4.join()\n",
    "        p5.join()\n",
    "        p6.join()\n",
    "        p7.join()\n",
    "        \n",
    "        # todos os processos finalizados\n",
    "        print(\"Terminado!\")\n",
    "t2 = time.time()\n",
    "time_elapsed = t2 -t1/3600\n",
    "print(\"tempo: \",time_elapsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot our performance over time.\n",
    "fig, ax = plt.subplots(figsize=(8.5, 6), dpi=130)\n",
    "\n",
    "ax.plot(performance_history_knn,color=\"blue\")\n",
    "ax.plot(performance_history_rf,color=\"red\")\n",
    "ax.plot(performance_history_tree,color=\"green\")\n",
    "ax.plot(performance_history_mlp,color=\"yellow\")\n",
    "ax.plot(performance_history_xgb,color=\"orange\")\n",
    "ax.plot(performance_history_svm,color=\"brown\")\n",
    "ax.plot(performance_history_nb,color=\"pink\")\n",
    "\n",
    "ax.scatter(range(len(performance_history_knn)), performance_history_knn,s=0)\n",
    "ax.scatter(range(len(performance_history_rf)), performance_history_rf, s=0)\n",
    "ax.scatter(range(len(performance_history_tree)), performance_history_tree,s=0)\n",
    "ax.scatter(range(len(performance_history_mlp)), performance_history_mlp,s=0)\n",
    "ax.scatter(range(len(performance_history_xgb)), performance_history_xgb,s=0)\n",
    "ax.scatter(range(len(performance_history_svm)), performance_history_svm,s=0)\n",
    "ax.scatter(range(len(performance_history_nb)), performance_history_nb,s=0)\n",
    "\n",
    "ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=5, integer=True))\n",
    "ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=10))\n",
    "ax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=1))\n",
    "\n",
    "ax.set_ylim(bottom=0, top=1)\n",
    "ax.grid(True)\n",
    "\n",
    "ax.set_title('Incremental classification accuracy')\n",
    "ax.set_xlabel('Query iteration')\n",
    "ax.set_ylabel('Classification Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
