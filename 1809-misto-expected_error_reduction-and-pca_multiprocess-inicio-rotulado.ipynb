{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from numpy import array, savetxt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carrega o dataset\n",
    "dados = pd.read_csv('/home/erasmor/Downloads/2017/todos_apenas_baixa_representatividade.csv',sep=\",\",encoding = 'utf-8',  header=0,na_values='.',dtype={'Label':'category'})\n",
    "\n",
    "#remove valores infinitos\n",
    "dados.replace(-np.Inf, np.nan)\n",
    "\n",
    "#substitui valores NaN\n",
    "dados.fillna(dados.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifica quantas instâncias (linhas) e quantos atributos (colunas) a base de dados contém\n",
    "print(\"numero de linhas e colunas: \",dados.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizar distribições por classes contidas no csv - informar nome da classe alvo\n",
    "print(dados.groupby('Label').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_raw_normalize = MinMaxScaler(X_raw_normalize.reshape(0, 1)).reshape(len(X_raw_normalize))\n",
    "#X_raw_normalizetd2 = (X_raw_normalize - X_raw_normalize.min(axis=0)) / (X_raw_normalize.max(axis=0) - X_raw_normalize.min(axis=0))\n",
    "# Obtendo os nomes das colunas do DataFrame como uma lista.\n",
    "cols = list(dados.columns)\n",
    "# colunas que nao serao normalizadas\n",
    "cols.remove('Label')\n",
    "\n",
    "# Copiando os dados e aplicando a normalizacao por reescala nas colunas do DataFrame que contem\n",
    "# valores continuos. Por padrao, o metodo minmax_scale reescala com min=0 e max=1.\n",
    "dados = dados[~dados.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "dados[cols] = dados[cols].apply(minmax_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define as colunas de atributos e a coluna da classe (de 0 a 72 são atributos e após a 72 é a classe)\n",
    "# \"X_raw\" é features/atributos e \"y_raw\" é target/classe ==> As duas formas abaixo dão certo.\n",
    "#array = dataset.values\n",
    "#X_raw = array[:,0:72]\n",
    "#y_raw = array[:,72]\n",
    "X_raw = dados.iloc[:, :-1].values # atributos\n",
    "y_raw = dados.iloc[:, 78].values # classe de ataques\n",
    "X_raw = np.nan_to_num(X_raw.astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformar a variável Y com valores categóricos das classses de ataques em valores:\n",
    "labelencoder_y = LabelEncoder()\n",
    "y_raw = labelencoder_y.fit_transform(y_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando um PCA. O parametro n_components indica a quantidade de dimensoes que a base\n",
    "# original sera reduzida.\n",
    "pca = PCA(n_components=10, whiten=True,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando o pca na base de dados. O atributo 'values' retorna um numpy.array\n",
    "# de duas dimensões (matriz) contendo apenas os valores numericos do DataFrame.\n",
    "X_raw = pca.fit_transform(X_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_error_reduction_function_knn(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, t_inicial):\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "    import functools\n",
    "    from IPython.display import clear_output\n",
    "    from copy import deepcopy\n",
    "    \n",
    "    #indica index da dobra para uso nos arquivos\n",
    "    indica_pool=str(idx_dobra)\n",
    "    \n",
    "    # recupera as amostras de treino iniciais - a extratificação realizada só serve para tal finalidade.\n",
    "    # No caso força a buscar pelo menos uma amostras de cada rótulo disponível (train_size= len(np.unique(y_raw)).\n",
    "    # Realizar a busca aleatoriamente não garante iniciar com uma instância de cada classe.\n",
    "    X_train_inicial, X_test_inicial, y_train_inicial, y_test_inicial = train_test_split(X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]], train_size= len(np.unique(y_raw[idx_data[idx_dobra][TRAIN]])) + t_inicial, stratify = y_raw[idx_data[idx_dobra][TRAIN]])\n",
    "    #print(\"tamanho de X_train inicial: \",X_train_inicial.shape,\" tamanho de y_train inicial: \",y_train_inicial.shape)\n",
    "    #print(y_train_inicial)  \n",
    "    # recupera amostras de teste de acordo com a dobra em uso\n",
    "    X_teste, y_teste = X_raw[idx_data[idx_dobra][TEST]], y_raw[idx_data[idx_dobra][TEST]]\n",
    "    # recupera amostras de treino (será o pool) de acordo com a dobra em uso\n",
    "    X_pool, y_pool = X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]]\n",
    "    \n",
    "    #isola exemplos rotulados para o treinamento inicial\n",
    "    X_train = X_train_inicial\n",
    "    y_train = y_train_inicial\n",
    "    \n",
    "    #instanciando classificadores de aprendizado ativo\n",
    "    learner_knn = ActiveLearner(estimator=KNeighborsClassifier(n_neighbors=5),X_training=X_train, y_training=y_train)\n",
    "    arquivo_performance_knn = open(\"expected_error_reduction_performance_knn_dobra_\"+indica_pool+\".txt\",\"a\")\n",
    "    arquivo_history_knn = (\"expected_error_reduction_history_knn_dobra_\"+indica_pool+\".csv\")\n",
    "        \n",
    "    #verifica a performance inicial\n",
    "    unqueried_score_knn = learner_knn.score(X_teste, y_teste)\n",
    "    predictions = learner_knn.predict(X_teste)\n",
    "    performance_history_knn.append(unqueried_score_knn)\n",
    "    \n",
    "    #inicio aprendizado ativo\n",
    " \n",
    "    for index in range(N_QUERIES):\n",
    "        n_labeled_examples_news = X_pool.shape[0]\n",
    "        training_indices_news = np.random.randint(low=0, high=n_labeled_examples_news, size=BATCH_SIZE)\n",
    "        amostra_recuperada_X = X_pool[training_indices_news]\n",
    "        amostra_recuperada_y = y_pool[training_indices_news]\n",
    "        X_pool = np.delete(X_pool, training_indices_news, axis=0)\n",
    "        y_pool = np.delete(y_pool, training_indices_news, axis=0)\n",
    "        if expected_error_reduction(learner_knn, X=amostra_recuperada_X) >= 0:\n",
    "            #Ensina ao modelo ActiveLearner as amostras recuperadas (amostras vão para o topo).\n",
    "            #learner_knn.teach(X=amostra_recuperada_X,y=amostra_recuperada_y)\n",
    "            learner_knn.teach(X=X_pool[training_indices_news].reshape(BATCH_SIZE, -1),y=y_pool[training_indices_news].reshape(BATCH_SIZE, ))\n",
    "        \n",
    "            new_score_knn = learner_knn.score(X_teste, y_teste)\n",
    "            performance_history_knn.append(new_score_knn)\n",
    "            clear_output(wait=True)\n",
    "            print ('Accuracy KNN after query no. %d: %f' % (index+1, new_score_knn))\n",
    "            arquivo_performance_knn.write('Accuracy after query no. %d: %f \\n' % (index+1,new_score_knn))\n",
    "            predictions = learner_knn.predict(X_teste)\n",
    "            #print ('Precision after query no. %d: %f' % (index+1, precision_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "            arquivo_performance_knn.write('Precision after query no. %d: %f \\n' % (index+1,precision_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "            #print ('Recall after query no. %d: %f' % (index+1, recall_score(y_test, predictions, average='macro',zero_division=1)))\n",
    "            arquivo_performance_knn.write('Recall after query no. %d: %f \\n' % (index+1, recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "            #print ('F1 score after query no. %d: %f' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "            #arquivo_performance.write('F1 score after query no. %d: %f \\n' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "            f1score= 2*((precision_score(y_teste, predictions,average='macro',zero_division=1)*recall_score(y_teste, predictions, average='macro',zero_division=1))/(precision_score(y_teste, predictions,average='macro',zero_division=1)+recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "            arquivo_performance_knn.write('F1 score after query no. %d: %f \\n' % (index+1, f1score))\n",
    "            #print (\"========================================\")\n",
    "            arquivo_performance_knn.write('======================================== \\n')\n",
    "                      \n",
    "            \n",
    "    arquivo_performance_knn.write(\"\\n Avaliação final KNN \\n\")\n",
    "    arquivo_performance_knn.write(classification_report(y_teste, predictions,zero_division=1))\n",
    "    np.savetxt(arquivo_history_knn, performance_history_knn,delimiter=\",\")\n",
    "    arquivo_performance_knn.close()\n",
    "\n",
    "def expected_error_reduction_function_rf(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, t_inicial):\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "    import functools\n",
    "    from IPython.display import clear_output\n",
    "    from copy import deepcopy\n",
    "    \n",
    "    #indica index da dobra para uso nos arquivos\n",
    "    indica_pool=str(idx_dobra)\n",
    "    \n",
    "    # recupera as amostras de treino iniciais - a extratificação realizada só serve para tal finalidade.\n",
    "    # No caso força a buscar pelo menos uma amostras de cada rótulo disponível (train_size= len(np.unique(y_raw)).\n",
    "    # Realizar a busca aleatoriamente não garante iniciar com uma instância de cada classe.\n",
    "    X_train_inicial, X_test_inicial, y_train_inicial, y_test_inicial = train_test_split(X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]], train_size= len(np.unique(y_raw[idx_data[idx_dobra][TRAIN]])) + t_inicial, stratify = y_raw[idx_data[idx_dobra][TRAIN]])\n",
    "    #print(\"tamanho de X_train inicial: \",X_train_inicial.shape,\" tamanho de y_train inicial: \",y_train_inicial.shape)\n",
    "    #print(y_train_inicial)  \n",
    "    # recupera amostras de teste de acordo com a dobra em uso\n",
    "    X_teste, y_teste = X_raw[idx_data[idx_dobra][TEST]], y_raw[idx_data[idx_dobra][TEST]]\n",
    "    # recupera amostras de treino (será o pool) de acordo com a dobra em uso\n",
    "    X_pool, y_pool = X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]]\n",
    "    \n",
    "    #isola exemplos rotulados para o treinamento inicial\n",
    "    X_train = X_train_inicial\n",
    "    y_train = y_train_inicial\n",
    "    \n",
    "    #instanciando classificadores de aprendizado ativo\n",
    "    learner_rf = ActiveLearner(estimator=RandomForestClassifier(random_state=42),X_training=X_train, y_training=y_train)\n",
    "    arquivo_performance_rf = open(\"expected_error_reduction_performance_rf_dobra_\"+indica_pool+\".txt\",\"a\")\n",
    "    arquivo_history_rf = (\"expected_error_reduction_history_rf_dobra_\"+indica_pool+\".csv\")\n",
    "    \n",
    "    #verifica a performance inicial\n",
    "    unqueried_score_rf = learner_rf.score(X_teste, y_teste)\n",
    "    predictions = learner_rf.predict(X_teste)\n",
    "    performance_history_rf.append(unqueried_score_rf)\n",
    "    \n",
    "    for index in range(N_QUERIES):\n",
    "        n_labeled_examples_news = X_pool.shape[0]\n",
    "        training_indices_news = np.random.randint(low=0, high=n_labeled_examples_news, size=BATCH_SIZE)\n",
    "        amostra_recuperada_X = X_pool[training_indices_news]\n",
    "        amostra_recuperada_y = y_pool[training_indices_news]\n",
    "        X_pool = np.delete(X_pool, training_indices_news, axis=0)\n",
    "        y_pool = np.delete(y_pool, training_indices_news, axis=0)\n",
    "        if expected_error_reduction(learner_rf, X=amostra_recuperada_X) >= 0:\n",
    "            #Ensina ao modelo ActiveLearner as amostras recuperadas (amostras vão para o topo).\n",
    "            #learner_rf.teach(X=amostra_recuperada_X,y=amostra_recuperada_y)\n",
    "            learner_rf.teach(X=X_pool[training_indices_news].reshape(BATCH_SIZE, -1),y=y_pool[training_indices_news].reshape(BATCH_SIZE, ))\n",
    "        \n",
    "            new_score_rf = learner_rf.score(X_teste, y_teste)\n",
    "            performance_history_rf.append(new_score_rf)\n",
    "            clear_output(wait=True)\n",
    "            print ('Accuracy RF after query no. %d: %f' % (index+1, new_score_rf))\n",
    "            arquivo_performance_rf.write('Accuracy after query no. %d: %f \\n' % (index+1,new_score_rf))\n",
    "            predictions = learner_rf.predict(X_teste)\n",
    "            #print ('Precision after query no. %d: %f' % (index+1, precision_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "            arquivo_performance_rf.write('Precision after query no. %d: %f \\n' % (index+1,precision_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "            #print ('Recall after query no. %d: %f' % (index+1, recall_score(y_test, predictions, average='macro',zero_division=1)))\n",
    "            arquivo_performance_rf.write('Recall after query no. %d: %f \\n' % (index+1, recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "            #print ('F1 score after query no. %d: %f' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "            #arquivo_performance.write('F1 score after query no. %d: %f \\n' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "            f1score= 2*((precision_score(y_teste, predictions,average='macro',zero_division=1)*recall_score(y_teste, predictions, average='macro',zero_division=1))/(precision_score(y_teste, predictions,average='macro',zero_division=1)+recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "            arquivo_performance_rf.write('F1 score after query no. %d: %f \\n' % (index+1, f1score))\n",
    "            #print (\"========================================\")\n",
    "            arquivo_performance_rf.write('======================================== \\n')\n",
    "            \n",
    "            \n",
    "    arquivo_performance_rf.write(\"\\n Avaliação final RF \\n\")\n",
    "    arquivo_performance_rf.write(classification_report(y_teste, predictions,zero_division=1))\n",
    "    np.savetxt(arquivo_history_rf, performance_history_rf,delimiter=\",\")\n",
    "    arquivo_performance_rf.close()\n",
    "    \n",
    "def expected_error_reduction_function_tree(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, t_inicial):\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "    import functools\n",
    "    from IPython.display import clear_output\n",
    "    from copy import deepcopy\n",
    "    \n",
    "    #indica index da dobra para uso nos arquivos\n",
    "    indica_pool=str(idx_dobra)\n",
    "    \n",
    "    # recupera as amostras de treino iniciais - a extratificação realizada só serve para tal finalidade.\n",
    "    # No caso força a buscar pelo menos uma amostras de cada rótulo disponível (train_size= len(np.unique(y_raw)).\n",
    "    # Realizar a busca aleatoriamente não garante iniciar com uma instância de cada classe.\n",
    "    X_train_inicial, X_test_inicial, y_train_inicial, y_test_inicial = train_test_split(X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]], train_size= len(np.unique(y_raw[idx_data[idx_dobra][TRAIN]])) + t_inicial, stratify = y_raw[idx_data[idx_dobra][TRAIN]])\n",
    "    #print(\"tamanho de X_train inicial: \",X_train_inicial.shape,\" tamanho de y_train inicial: \",y_train_inicial.shape)\n",
    "    #print(y_train_inicial)  \n",
    "    # recupera amostras de teste de acordo com a dobra em uso\n",
    "    X_teste, y_teste = X_raw[idx_data[idx_dobra][TEST]], y_raw[idx_data[idx_dobra][TEST]]\n",
    "    # recupera amostras de treino (será o pool) de acordo com a dobra em uso\n",
    "    X_pool, y_pool = X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]]\n",
    "    \n",
    "    #isola exemplos rotulados para o treinamento inicial\n",
    "    X_train = X_train_inicial\n",
    "    y_train = y_train_inicial\n",
    "    \n",
    "    #instanciando classificadores de aprendizado ativo\n",
    "    learner_tree =ActiveLearner(estimator=DecisionTreeClassifier(),X_training=X_train, y_training=y_train)\n",
    "    arquivo_performance_tree = open(\"expected_error_reduction_performance_tree_dobra_\"+indica_pool+\".txt\",\"a\")\n",
    "    arquivo_history_tree = (\"expected_error_reduction_history_tree_dobra_\"+indica_pool+\".csv\")\n",
    "    \n",
    "    #verifica a performance inicial\n",
    "    unqueried_score_tree = learner_tree.score(X_teste, y_teste)\n",
    "    predictions = learner_tree.predict(X_teste)\n",
    "    performance_history_tree.append(unqueried_score_tree)\n",
    "    \n",
    "    for index in range(N_QUERIES):\n",
    "        n_labeled_examples_news = X_pool.shape[0]\n",
    "        training_indices_news = np.random.randint(low=0, high=n_labeled_examples_news, size=BATCH_SIZE)\n",
    "        amostra_recuperada_X = X_pool[training_indices_news]\n",
    "        amostra_recuperada_y = y_pool[training_indices_news]\n",
    "        X_pool = np.delete(X_pool, training_indices_news, axis=0)\n",
    "        y_pool = np.delete(y_pool, training_indices_news, axis=0)\n",
    "        if expected_error_reduction(learner_tree, X=amostra_recuperada_X) >= 0:\n",
    "            #Ensina ao modelo ActiveLearner as amostras recuperadas (amostras vão para o topo).\n",
    "            #learner_tree.teach(X=amostra_recuperada_X,y=amostra_recuperada_y)\n",
    "            learner_tree.teach(X=X_pool[training_indices_news].reshape(BATCH_SIZE, -1),y=y_pool[training_indices_news].reshape(BATCH_SIZE, ))\n",
    "            new_score_tree = learner_tree.score(X_teste, y_teste)\n",
    "            performance_history_tree.append(new_score_tree)\n",
    "            clear_output(wait=True)\n",
    "            print ('Accuracy TREE after query no. %d: %f' % (index+1, new_score_tree))\n",
    "            arquivo_performance_tree.write('Accuracy after query no. %d: %f \\n' % (index+1,new_score_tree))\n",
    "            predictions = learner_tree.predict(X_teste)\n",
    "            #print ('Precision after query no. %d: %f' % (index+1, precision_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "            arquivo_performance_tree.write('Precision after query no. %d: %f \\n' % (index+1,precision_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "            #print ('Recall after query no. %d: %f' % (index+1, recall_score(y_test, predictions, average='macro',zero_division=1)))\n",
    "            arquivo_performance_tree.write('Recall after query no. %d: %f \\n' % (index+1, recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "            #print ('F1 score after query no. %d: %f' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "            #arquivo_performance.write('F1 score after query no. %d: %f \\n' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "            f1score= 2*((precision_score(y_teste, predictions,average='macro',zero_division=1)*recall_score(y_teste, predictions, average='macro',zero_division=1))/(precision_score(y_teste, predictions,average='macro',zero_division=1)+recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "            arquivo_performance_tree.write('F1 score after query no. %d: %f \\n' % (index+1, f1score))\n",
    "            #print (\"========================================\")\n",
    "            arquivo_performance_tree.write('======================================== \\n')\n",
    "            \n",
    "            \n",
    "    arquivo_performance_tree.write(\"\\n Avaliação final TREE \\n\")\n",
    "    arquivo_performance_tree.write(classification_report(y_teste, predictions,zero_division=1))\n",
    "    np.savetxt(arquivo_history_tree, performance_history_tree,delimiter=\",\")\n",
    "    arquivo_performance_tree.close()\n",
    "\n",
    "def expected_error_reduction_function_mlp(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, t_inicial):\n",
    "    \n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "    import functools\n",
    "    from IPython.display import clear_output\n",
    "    from copy import deepcopy\n",
    "    \n",
    "    #indica index da dobra para uso nos arquivos\n",
    "    indica_pool=str(idx_dobra)\n",
    "    \n",
    "    # recupera as amostras de treino iniciais - a extratificação realizada só serve para tal finalidade.\n",
    "    # No caso força a buscar pelo menos uma amostras de cada rótulo disponível (train_size= len(np.unique(y_raw)).\n",
    "    # Realizar a busca aleatoriamente não garante iniciar com uma instância de cada classe.\n",
    "    X_train_inicial, X_test_inicial, y_train_inicial, y_test_inicial = train_test_split(X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]], train_size= len(np.unique(y_raw[idx_data[idx_dobra][TRAIN]])) + t_inicial, stratify = y_raw[idx_data[idx_dobra][TRAIN]])\n",
    "    #print(\"tamanho de X_train inicial: \",X_train_inicial.shape,\" tamanho de y_train inicial: \",y_train_inicial.shape)\n",
    "    #print(y_train_inicial)  \n",
    "    # recupera amostras de teste de acordo com a dobra em uso\n",
    "    X_teste, y_teste = X_raw[idx_data[idx_dobra][TEST]], y_raw[idx_data[idx_dobra][TEST]]\n",
    "    # recupera amostras de treino (será o pool) de acordo com a dobra em uso\n",
    "    X_pool, y_pool = X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]]\n",
    "    \n",
    "    #isola exemplos rotulados para o treinamento inicial\n",
    "    X_train = X_train_inicial\n",
    "    y_train = y_train_inicial\n",
    "    \n",
    "    #instanciando classificadores de aprendizado ativo\n",
    "    learner_mlp = ActiveLearner(estimator=MLPClassifier(max_iter = 2000),X_training=X_train, y_training=y_train)\n",
    "    arquivo_performance_mlp = open(\"expected_error_reduction_performance_mlp_dobra_\"+indica_pool+\".txt\",\"a\")\n",
    "    arquivo_history_mlp = (\"expected_error_reduction_history_mlp_dobra_\"+indica_pool+\".csv\")\n",
    "        \n",
    "    #verifica a performance inicial\n",
    "    unqueried_score_mlp = learner_mlp.score(X_teste, y_teste)\n",
    "    predictions = learner_mlp.predict(X_teste)\n",
    "    performance_history_mlp.append(unqueried_score_mlp)\n",
    "    \n",
    "  \n",
    "    for index in range(N_QUERIES):\n",
    "        n_labeled_examples_news = X_pool.shape[0]\n",
    "        training_indices_news = np.random.randint(low=0, high=n_labeled_examples_news, size=BATCH_SIZE)\n",
    "        amostra_recuperada_X = X_pool[training_indices_news]\n",
    "        amostra_recuperada_y = y_pool[training_indices_news]\n",
    "        X_pool = np.delete(X_pool, training_indices_news, axis=0)\n",
    "        y_pool = np.delete(y_pool, training_indices_news, axis=0)\n",
    "        if expected_error_reduction(learner_mlp, X=amostra_recuperada_X) >= 0:\n",
    "            #Ensina ao modelo ActiveLearner as amostras recuperadas (amostras vão para o topo).\n",
    "            #learner_mlp.teach(X=amostra_recuperada_X,y=amostra_recuperada_y)\n",
    "            learner_mlp.teach(X=X_pool[training_indices_news].reshape(BATCH_SIZE, -1),y=y_pool[training_indices_news].reshape(BATCH_SIZE, ))\n",
    "            new_score_mlp = learner_mlp.score(X_teste, y_teste)\n",
    "            performance_history_mlp.append(new_score_mlp)\n",
    "            clear_output(wait=True)\n",
    "            print ('Accuracy MLP after query no. %d: %f' % (index+1, new_score_mlp))\n",
    "            arquivo_performance_mlp.write('Accuracy after query no. %d: %f \\n' % (index+1,new_score_mlp))\n",
    "            predictions = learner_mlp.predict(X_teste)\n",
    "            #print ('Precision after query no. %d: %f' % (index+1, precision_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "            arquivo_performance_mlp.write('Precision after query no. %d: %f \\n' % (index+1,precision_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "            #print ('Recall after query no. %d: %f' % (index+1, recall_score(y_test, predictions, average='macro',zero_division=1)))\n",
    "            arquivo_performance_mlp.write('Recall after query no. %d: %f \\n' % (index+1, recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "            #print ('F1 score after query no. %d: %f' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "            #arquivo_performance.write('F1 score after query no. %d: %f \\n' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "            f1score= 2*((precision_score(y_teste, predictions,average='macro',zero_division=1)*recall_score(y_teste, predictions, average='macro',zero_division=1))/(precision_score(y_teste, predictions,average='macro',zero_division=1)+recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "            arquivo_performance_mlp.write('F1 score after query no. %d: %f \\n' % (index+1, f1score))\n",
    "            #print (\"========================================\")\n",
    "            arquivo_performance_mlp.write('======================================== \\n')\n",
    "            \n",
    "            \n",
    "    arquivo_performance_mlp.write(\"\\n Avaliação final MLP \\n\")\n",
    "    arquivo_performance_mlp.write(classification_report(y_teste, predictions,zero_division=1))\n",
    "    np.savetxt(arquivo_history_mlp, performance_history_mlp,delimiter=\",\")\n",
    "    arquivo_performance_mlp.close()\n",
    "\n",
    "def expected_error_reduction_function_xgb(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, t_inicial):\n",
    "    \n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "    import functools\n",
    "    from IPython.display import clear_output\n",
    "    from copy import deepcopy\n",
    "    \n",
    "    #indica index da dobra para uso nos arquivos\n",
    "    indica_pool=str(idx_dobra)\n",
    "    \n",
    "    # recupera as amostras de treino iniciais - a extratificação realizada só serve para tal finalidade.\n",
    "    # No caso força a buscar pelo menos uma amostras de cada rótulo disponível (train_size= len(np.unique(y_raw)).\n",
    "    # Realizar a busca aleatoriamente não garante iniciar com uma instância de cada classe.\n",
    "    X_train_inicial, X_test_inicial, y_train_inicial, y_test_inicial = train_test_split(X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]], train_size= len(np.unique(y_raw[idx_data[idx_dobra][TRAIN]])) + t_inicial, stratify = y_raw[idx_data[idx_dobra][TRAIN]])\n",
    "    #print(\"tamanho de X_train inicial: \",X_train_inicial.shape,\" tamanho de y_train inicial: \",y_train_inicial.shape)\n",
    "    #print(y_train_inicial)  \n",
    "    # recupera amostras de teste de acordo com a dobra em uso\n",
    "    X_teste, y_teste = X_raw[idx_data[idx_dobra][TEST]], y_raw[idx_data[idx_dobra][TEST]]\n",
    "    # recupera amostras de treino (será o pool) de acordo com a dobra em uso\n",
    "    X_pool, y_pool = X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]]\n",
    "    \n",
    "    #isola exemplos rotulados para o treinamento inicial\n",
    "    X_train = X_train_inicial\n",
    "    y_train = y_train_inicial\n",
    "    \n",
    "    #instanciando classificadores de aprendizado ativo\n",
    "    learner_xgb = ActiveLearner(estimator=GradientBoostingClassifier(n_estimators=7, learning_rate=1.0,max_depth=1, random_state=42),X_training=X_train, y_training=y_train)\n",
    "    arquivo_performance_xgb = open(\"expected_error_reduction_performance_xgb_dobra_\"+indica_pool+\".txt\",\"a\")\n",
    "    arquivo_history_xgb = (\"expected_error_reduction_history_xgb_dobra_\"+indica_pool+\".csv\")\n",
    "        \n",
    "    #verifica a performance inicial\n",
    "    unqueried_score_xgb = learner_xgb.score(X_teste, y_teste)\n",
    "    predictions = learner_xgb.predict(X_teste)\n",
    "    performance_history_xgb.append(unqueried_score_xgb)\n",
    "    \n",
    "  \n",
    "    for index in range(N_QUERIES):\n",
    "        n_labeled_examples_news = X_pool.shape[0]\n",
    "        training_indices_news = np.random.randint(low=0, high=n_labeled_examples_news, size=BATCH_SIZE)\n",
    "        amostra_recuperada_X = X_pool[training_indices_news]\n",
    "        amostra_recuperada_y = y_pool[training_indices_news]\n",
    "        X_pool = np.delete(X_pool, training_indices_news, axis=0)\n",
    "        y_pool = np.delete(y_pool, training_indices_news, axis=0)\n",
    "        if expected_error_reduction(learner_xgb, X=amostra_recuperada_X) >= 0:\n",
    "            #Ensina ao modelo ActiveLearner as amostras recuperadas (amostras vão para o topo).\n",
    "            #learner_xgb.teach(X=amostra_recuperada_X,y=amostra_recuperada_y)\n",
    "            learner_xgb.teach(X=X_pool[training_indices_news].reshape(BATCH_SIZE, -1),y=y_pool[training_indices_news].reshape(BATCH_SIZE, ))\n",
    "            new_score_xgb = learner_xgb.score(X_teste, y_teste)\n",
    "            performance_history_xgb.append(new_score_xgb)\n",
    "            clear_output(wait=True)\n",
    "            print ('Accuracy XGB after query no. %d: %f' % (index+1, new_score_xgb))\n",
    "            arquivo_performance_xgb.write('Accuracy after query no. %d: %f \\n' % (index+1,new_score_xgb))\n",
    "            predictions = learner_xgb.predict(X_teste)\n",
    "            #print ('Precision after query no. %d: %f' % (index+1, precision_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "            arquivo_performance_xgb.write('Precision after query no. %d: %f \\n' % (index+1,precision_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "            #print ('Recall after query no. %d: %f' % (index+1, recall_score(y_test, predictions, average='macro',zero_division=1)))\n",
    "            arquivo_performance_xgb.write('Recall after query no. %d: %f \\n' % (index+1, recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "            #print ('F1 score after query no. %d: %f' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "            #arquivo_performance.write('F1 score after query no. %d: %f \\n' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "            f1score= 2*((precision_score(y_teste, predictions,average='macro',zero_division=1)*recall_score(y_teste, predictions, average='macro',zero_division=1))/(precision_score(y_teste, predictions,average='macro',zero_division=1)+recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "            arquivo_performance_xgb.write('F1 score after query no. %d: %f \\n' % (index+1, f1score))\n",
    "            #print (\"========================================\")\n",
    "            arquivo_performance_xgb.write('======================================== \\n')\n",
    "            \n",
    "            \n",
    "    arquivo_performance_xgb.write(\"\\n Avaliação final XGB \\n\")\n",
    "    arquivo_performance_xgb.write(classification_report(y_teste, predictions,zero_division=1))\n",
    "    np.savetxt(arquivo_history_xgb, performance_history_xgb,delimiter=\",\")\n",
    "    arquivo_performance_xgb.close()\n",
    "\n",
    "def expected_error_reduction_function_svm(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, t_inicial):\n",
    "    \n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn import svm\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "    import functools\n",
    "    from IPython.display import clear_output\n",
    "    from copy import deepcopy\n",
    "    \n",
    "    #indica index da dobra para uso nos arquivos\n",
    "    indica_pool=str(idx_dobra)\n",
    "    \n",
    "    # recupera as amostras de treino iniciais - a extratificação realizada só serve para tal finalidade.\n",
    "    # No caso força a buscar pelo menos uma amostras de cada rótulo disponível (train_size= len(np.unique(y_raw)).\n",
    "    # Realizar a busca aleatoriamente não garante iniciar com uma instância de cada classe.\n",
    "    X_train_inicial, X_test_inicial, y_train_inicial, y_test_inicial = train_test_split(X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]], train_size= len(np.unique(y_raw[idx_data[idx_dobra][TRAIN]])) + t_inicial, stratify = y_raw[idx_data[idx_dobra][TRAIN]])\n",
    "    #print(\"tamanho de X_train inicial: \",X_train_inicial.shape,\" tamanho de y_train inicial: \",y_train_inicial.shape)\n",
    "    #print(y_train_inicial)  \n",
    "    # recupera amostras de teste de acordo com a dobra em uso\n",
    "    X_teste, y_teste = X_raw[idx_data[idx_dobra][TEST]], y_raw[idx_data[idx_dobra][TEST]]\n",
    "    # recupera amostras de treino (será o pool) de acordo com a dobra em uso\n",
    "    X_pool, y_pool = X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]]\n",
    "    \n",
    "    #isola exemplos rotulados para o treinamento inicial\n",
    "    X_train = X_train_inicial\n",
    "    y_train = y_train_inicial\n",
    "    \n",
    "    #instanciando classificadores de aprendizado ativo\n",
    "    learner_svm = ActiveLearner(estimator=svm.SVC(kernel='linear',probability=True),X_training=X_train, y_training=y_train)\n",
    "    arquivo_performance_svm = open(\"expected_error_reduction_performance_svm_dobra_\"+indica_pool+\".txt\",\"a\")\n",
    "    arquivo_history_svm = (\"expected_error_reduction_history_svm_dobra_\"+indica_pool+\".csv\")\n",
    "        \n",
    "    #verifica a performance inicial\n",
    "    unqueried_score_svm = learner_svm.score(X_teste, y_teste)\n",
    "    predictions = learner_svm.predict(X_teste)\n",
    "    performance_history_svm.append(unqueried_score_svm)\n",
    "    \n",
    "  \n",
    "    for index in range(N_QUERIES):\n",
    "        n_labeled_examples_news = X_pool.shape[0]\n",
    "        training_indices_news = np.random.randint(low=0, high=n_labeled_examples_news, size=BATCH_SIZE)\n",
    "        amostra_recuperada_X = X_pool[training_indices_news]\n",
    "        amostra_recuperada_y = y_pool[training_indices_news]\n",
    "        X_pool = np.delete(X_pool, training_indices_news, axis=0)\n",
    "        y_pool = np.delete(y_pool, training_indices_news, axis=0)\n",
    "        if expected_error_reduction(learner_svm, X=amostra_recuperada_X) >= 0:\n",
    "            #Ensina ao modelo ActiveLearner as amostras recuperadas (amostras vão para o topo).\n",
    "            #learner_svm.teach(X=amostra_recuperada_X,y=amostra_recuperada_y)\n",
    "            learner_svm.teach(X=X_pool[training_indices_news].reshape(BATCH_SIZE, -1),y=y_pool[training_indices_news].reshape(BATCH_SIZE, ))\n",
    "            new_score_svm = learner_svm.score(X_teste, y_teste)\n",
    "            performance_history_svm.append(new_score_svm)\n",
    "            clear_output(wait=True)\n",
    "            print ('Accuracy SVM after query no. %d: %f' % (index+1, new_score_svm))\n",
    "            arquivo_performance_svm.write('Accuracy after query no. %d: %f \\n' % (index+1,new_score_svm))\n",
    "            predictions = learner_svm.predict(X_teste)\n",
    "            #print ('Precision after query no. %d: %f' % (index+1, precision_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "            arquivo_performance_svm.write('Precision after query no. %d: %f \\n' % (index+1,precision_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "            #print ('Recall after query no. %d: %f' % (index+1, recall_score(y_test, predictions, average='macro',zero_division=1)))\n",
    "            arquivo_performance_svm.write('Recall after query no. %d: %f \\n' % (index+1, recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "            #print ('F1 score after query no. %d: %f' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "            #arquivo_performance.write('F1 score after query no. %d: %f \\n' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "            f1score= 2*((precision_score(y_teste, predictions,average='macro',zero_division=1)*recall_score(y_teste, predictions, average='macro',zero_division=1))/(precision_score(y_teste, predictions,average='macro',zero_division=1)+recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "            arquivo_performance_svm.write('F1 score after query no. %d: %f \\n' % (index+1, f1score))\n",
    "            #print (\"========================================\")\n",
    "            arquivo_performance_svm.write('======================================== \\n')\n",
    "            \n",
    "    arquivo_performance_svm.write(\"\\n Avaliação final SVM \\n\")\n",
    "    arquivo_performance_svm.write(classification_report(y_teste, predictions,zero_division=1))\n",
    "    np.savetxt(arquivo_history_svm, performance_history_svm,delimiter=\",\")\n",
    "    arquivo_performance_svm.close()\n",
    "\n",
    "def expected_error_reduction_function_nb(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, t_inicial):\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "    import functools\n",
    "    from IPython.display import clear_output\n",
    "    from copy import deepcopy\n",
    "    \n",
    "    #indica index da dobra para uso nos arquivos\n",
    "    indica_pool=str(idx_dobra)\n",
    "    \n",
    "    # recupera as amostras de treino iniciais - a extratificação realizada só serve para tal finalidade.\n",
    "    # No caso força a buscar pelo menos uma amostras de cada rótulo disponível (train_size= len(np.unique(y_raw)).\n",
    "    # Realizar a busca aleatoriamente não garante iniciar com uma instância de cada classe.\n",
    "    X_train_inicial, X_test_inicial, y_train_inicial, y_test_inicial = train_test_split(X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]], train_size= len(np.unique(y_raw[idx_data[idx_dobra][TRAIN]])) + t_inicial, stratify = y_raw[idx_data[idx_dobra][TRAIN]])\n",
    "    #print(\"tamanho de X_train inicial: \",X_train_inicial.shape,\" tamanho de y_train inicial: \",y_train_inicial.shape)\n",
    "    #print(y_train_inicial)  \n",
    "    # recupera amostras de teste de acordo com a dobra em uso\n",
    "    X_teste, y_teste = X_raw[idx_data[idx_dobra][TEST]], y_raw[idx_data[idx_dobra][TEST]]\n",
    "    # recupera amostras de treino (será o pool) de acordo com a dobra em uso\n",
    "    X_pool, y_pool = X_raw[idx_data[idx_dobra][TRAIN]], y_raw[idx_data[idx_dobra][TRAIN]]\n",
    "    \n",
    "    #isola exemplos rotulados para o treinamento inicial\n",
    "    X_train = X_train_inicial\n",
    "    y_train = y_train_inicial\n",
    "    \n",
    "    #instanciando classificadores de aprendizado ativo\n",
    "    learner_nb = ActiveLearner(estimator=GaussianNB(),X_training=X_train, y_training=y_train)\n",
    "    arquivo_performance_nb = open(\"expected_error_reduction_performance_nb_dobra_\"+indica_pool+\".txt\",\"a\")\n",
    "    arquivo_history_nb = (\"expected_error_reduction_history_nb_dobra_\"+indica_pool+\".csv\")\n",
    "        \n",
    "    #verifica a performance inicial\n",
    "    unqueried_score_nb = learner_nb.score(X_teste, y_teste)\n",
    "    predictions = learner_nb.predict(X_teste)\n",
    "    performance_history_nb.append(unqueried_score_nb)\n",
    "    \n",
    "      #inicio aprendizado ativo\n",
    " \n",
    "    for index in range(N_QUERIES):\n",
    "        n_labeled_examples_news = X_pool.shape[0]\n",
    "        training_indices_news = np.random.randint(low=0, high=n_labeled_examples_news, size=BATCH_SIZE)\n",
    "        amostra_recuperada_X = X_pool[training_indices_news]\n",
    "        amostra_recuperada_y = y_pool[training_indices_news]\n",
    "        X_pool = np.delete(X_pool, training_indices_news, axis=0)\n",
    "        y_pool = np.delete(y_pool, training_indices_news, axis=0)\n",
    "        if expected_error_reduction(learner_nb, X=amostra_recuperada_X) >= 0:\n",
    "            #Ensina ao modelo ActiveLearner as amostras recuperadas (amostras vão para o topo).\n",
    "            #learner_nb.teach(X=amostra_recuperada_X,y=amostra_recuperada_y)\n",
    "            learner_nb.teach(X=X_pool[training_indices_news].reshape(BATCH_SIZE, -1),y=y_pool[training_indices_news].reshape(BATCH_SIZE, ))\n",
    "            new_score_nb = learner_nb.score(X_teste, y_teste)\n",
    "            performance_history_nb.append(new_score_nb)\n",
    "            clear_output(wait=True)\n",
    "            print ('Accuracy NB after query no. %d: %f' % (index+1, new_score_nb))\n",
    "            arquivo_performance_nb.write('Accuracy after query no. %d: %f \\n' % (index+1,new_score_nb))\n",
    "            predictions = learner_nb.predict(X_teste)\n",
    "            #print ('Precision after query no. %d: %f' % (index+1, precision_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "            arquivo_performance_nb.write('Precision after query no. %d: %f \\n' % (index+1,precision_score(y_teste, predictions,average='macro',zero_division=1)))\n",
    "            #print ('Recall after query no. %d: %f' % (index+1, recall_score(y_test, predictions, average='macro',zero_division=1)))\n",
    "            arquivo_performance_nb.write('Recall after query no. %d: %f \\n' % (index+1, recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "            #print ('F1 score after query no. %d: %f' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "            #arquivo_performance.write('F1 score after query no. %d: %f \\n' % (index+1, f1_score(y_test, predictions,average='macro',zero_division=1)))\n",
    "            f1score= 2*((precision_score(y_teste, predictions,average='macro',zero_division=1)*recall_score(y_teste, predictions, average='macro',zero_division=1))/(precision_score(y_teste, predictions,average='macro',zero_division=1)+recall_score(y_teste, predictions, average='macro',zero_division=1)))\n",
    "            arquivo_performance_nb.write('F1 score after query no. %d: %f \\n' % (index+1, f1score))\n",
    "            #print (\"========================================\")\n",
    "            arquivo_performance_nb.write('======================================== \\n')\n",
    "                      \n",
    "            \n",
    "    arquivo_performance_nb.write(\"\\n Avaliação final NB \\n\")\n",
    "    arquivo_performance_nb.write(classification_report(y_teste, predictions,zero_division=1))\n",
    "    np.savetxt(arquivo_history_nb, performance_history_nb,delimiter=\",\")\n",
    "    arquivo_performance_nb.close()\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit, train_test_split\n",
    "from modAL.uncertainty import classifier_uncertainty\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling\n",
    "from modAL.expected_error import expected_error_reduction\n",
    "from timeit import Timer\n",
    "import time\n",
    "import functools\n",
    "# importing the multiprocessing module\n",
    "import multiprocessing\n",
    "\n",
    "#inicia relogio\n",
    "t1 = time.time()\n",
    "# Define o tamanho das divisões feitas no dataset (cross-validation)\n",
    "n_dobras = 10\n",
    "# Define Tamanho inicial da amostra (toda estratégia parte de um tamanho mínimo aleatório).\n",
    "t_inicial = 10\n",
    "#define array de indices das partições\n",
    "idx_data =[]\n",
    "# cross validation bags - n_splits\n",
    "data_cv = StratifiedShuffleSplit(n_splits= n_dobras,random_state=42) \n",
    "data_cv.get_n_splits(X_raw, y_raw)\n",
    "# chame a instância e gere os dados sobre a base original\n",
    "type(data_cv.split(X_raw, y_raw))\n",
    "# dividir os dados - A função split.split () retorna índices para amostras de treino e amostras de teste. \n",
    "# Ele examinará o número de validação cruzada especificado e retornará cada vez que treinar \n",
    "# e testar os índices de amostra usando os conjuntos de dados de treinamento e teste que podem \n",
    "# ser criados filtrando o conjunto de dados inteiro. Por exemplo idx_data[0][1], o primeiro indice faz referencia\n",
    "# a dobra e o segundo indice faz referencia a posição da dobra (0 = treino e 1 = teste). Logo TRAIN=0 e TEST=1.\n",
    "for train_index, test_index in data_cv.split(X_raw,y_raw):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    #print(\"n_split\",n_splits,\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    idx_data.append([train_index, test_index])\n",
    "#verifica tamanho das dobras (numero de instâncias de cada dobra)\n",
    "#print(\"tamanho de cada dobra: \",idx_data[3][0].shape)\n",
    "\n",
    "\n",
    "TRAIN =0\n",
    "TEST =1\n",
    "\n",
    "#inicia arrays de performance\n",
    "performance_history_knn = []\n",
    "performance_history_rf = []\n",
    "performance_history_tree = []\n",
    "performance_history_mlp = []\n",
    "performance_history_xgb = []\n",
    "performance_history_svm = []\n",
    "performance_history_nb = []\n",
    "\n",
    "# Define numero de queries\n",
    "BATCH_SIZE = 4\n",
    "N_RAW_SAMPLES = 200\n",
    "N_QUERIES = N_RAW_SAMPLES // BATCH_SIZE\n",
    "#chama procedimento de aprendizado para todas as dobras\n",
    "for idx_dobra in range(1):\n",
    "    threading.Thread(target=expected_error_reduction_function_knn(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, t_inicial)).start()\n",
    "    threading.Thread(target=expected_error_reduction_function_rf(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, t_inicial)).start()\n",
    "    threading.Thread(target=expected_error_reduction_function_nb(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, t_inicial)).start()\n",
    "    #threading.Thread(target=expected_error_reduction_function_mlp(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, t_inicial)).start()\n",
    "    threading.Thread(target=expected_error_reduction_function_tree(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, t_inicial)).start()\n",
    "    #threading.Thread(target=expected_error_reduction_function_xgb(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, t_inicial)).start()\n",
    "    #threading.Thread(target=expected_error_reduction_function_svm(X_raw, y_raw, idx_data, idx_dobra, TRAIN, TEST, t_inicial)).start()\n",
    "\n",
    "       \n",
    "t2 = time.time()\n",
    "time_elapsed = (t2-t1)\n",
    "hours, rem = divmod(time_elapsed, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot our performance over time.\n",
    "fig, ax = plt.subplots(figsize=(8.5, 6), dpi=130)\n",
    "\n",
    "ax.plot(performance_history_knn,color=\"blue\")\n",
    "ax.plot(performance_history_rf,color=\"red\")\n",
    "ax.plot(performance_history_tree,color=\"green\")\n",
    "ax.plot(performance_history_mlp,color=\"yellow\")\n",
    "ax.plot(performance_history_xgb,color=\"orange\")\n",
    "ax.plot(performance_history_svm,color=\"brown\")\n",
    "ax.plot(performance_history_nb,color=\"pink\")\n",
    "\n",
    "ax.scatter(range(len(performance_history_knn)), performance_history_knn,s=0)\n",
    "ax.scatter(range(len(performance_history_rf)), performance_history_rf, s=0)\n",
    "ax.scatter(range(len(performance_history_tree)), performance_history_tree,s=0)\n",
    "ax.scatter(range(len(performance_history_mlp)), performance_history_mlp,s=0)\n",
    "ax.scatter(range(len(performance_history_xgb)), performance_history_xgb,s=0)\n",
    "ax.scatter(range(len(performance_history_svm)), performance_history_svm,s=0)\n",
    "ax.scatter(range(len(performance_history_nb)), performance_history_nb,s=0)\n",
    "\n",
    "ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=5, integer=True))\n",
    "ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=10))\n",
    "ax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=1))\n",
    "\n",
    "ax.set_ylim(bottom=0, top=1)\n",
    "ax.grid(True)\n",
    "\n",
    "ax.set_title('Incremental classification accuracy')\n",
    "ax.set_xlabel('Query iteration')\n",
    "ax.set_ylabel('Classification Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
